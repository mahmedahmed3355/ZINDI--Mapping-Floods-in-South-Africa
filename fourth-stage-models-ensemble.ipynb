{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10776345,"sourceType":"datasetVersion","datasetId":6682996}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nfrom glob import glob\nimport sys\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom scipy.stats import entropy\nfrom scipy.signal import butter, lfilter, freqz\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom tqdm.auto import tqdm\nfrom functools import partial\nimport cv2\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\nfrom sklearn.preprocessing import LabelEncoder\nfrom torchvision.transforms import v2\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A\nfrom albumentations import (Compose, Normalize, Resize, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate, Transpose)\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\nimport timm\nfrom scipy import optimize\nimport warnings \nwarnings.filterwarnings('ignore')\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom matplotlib import pyplot as plt\nimport joblib\nVERSION=16\nBASE_PATH = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:25.115582Z","iopub.execute_input":"2025-02-18T10:51:25.115956Z","iopub.status.idle":"2025-02-18T10:51:25.126246Z","shell.execute_reply.started":"2025-02-18T10:51:25.115923Z","shell.execute_reply":"2025-02-18T10:51:25.124965Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\n\nclass CFG:\n    wandb = True\n    debug = False\n    train=True\n    apex=True\n    t4_gpu=False\n    scheduler='OneCycleLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n    # CosineAnnealingLR params\n    cosanneal_params={\n        'T_max':6,\n        'eta_min':1e-5,\n        'last_epoch':-1\n    }\n    #ReduceLROnPlateau params\n    reduce_params={\n        'mode':'min',\n        'factor':0.2,\n        'patience':4,\n        'eps':1e-6,\n        'verbose':True\n    }\n    # CosineAnnealingWarmRestarts params\n    cosanneal_res_params={\n        'T_0':20,\n        'eta_min':1e-6,\n        'T_mult':1,\n        'last_epoch':-1\n    }\n    print_freq=5\n    num_workers = 1\n    cnn_model_name = 'resnet50d'\n    model_name = 'resnet50d_resnet1d_multimodal'\n    optimizer='Adan'\n    epochs = 25\n    factor = 0.9\n    patience = 2\n    eps = 1e-6\n    lr = 1e-3\n    min_lr = 1e-6\n    batch_size = 32\n    weight_decay = 1e-2\n    batch_scheduler=True\n    gradient_accumulation_steps = 1\n    max_grad_norm = 1e6\n    seed = 2025\n    tta = 5\n    target_cols = \"label\"\n    target_size = 1\n    in_channels = 1\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:41:45.620389Z","iopub.execute_input":"2025-02-18T10:41:45.620730Z","iopub.status.idle":"2025-02-18T10:41:45.627683Z","shell.execute_reply.started":"2025-02-18T10:41:45.620703Z","shell.execute_reply":"2025-02-18T10:41:45.626466Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def get_score(preds, targets):\n    \n    return log_loss(targets, preds)\n\ndef seed_torch(seed=CFG.seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.enabled = True\n    \nseed_torch(seed=CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:41:45.629724Z","iopub.execute_input":"2025-02-18T10:41:45.630097Z","iopub.status.idle":"2025-02-18T10:41:45.665178Z","shell.execute_reply.started":"2025-02-18T10:41:45.630067Z","shell.execute_reply":"2025-02-18T10:41:45.663992Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"##paths to the oofs from the subsequent notebooks\nresnet1d_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/resnet_1d_subs_and_oof/resnet1d_final_oof.csv\"\nlgbm_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/lightgbm_subs_and_oof/lgb_train_with_oof.csv\"\nfastai_tabular_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/fastai_subs_and_oof/fastai_train_with_oof.csv\"\nxgb_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/xgb_subs_and_oof/xgb_train_with_oof.csv\"\ntabconv_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/tabconv_subs_and_oof/tab_conv_oof.csv\"\ntabtrans_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/tabtransformer_subs_and_oof/transformer_tab_oof.csv\"\nfastai_gated_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/fastai_gated_cnn_subs_and_oof/fastai_Gated_CNN_train_with_oof.csv\"\nwavenet_gru_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/wavenet_subs_and_oof/wavenet_gru_transformer_oof_df_version9.csv\"\ntabnet_oof_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/tabnet_subs_and_oof/tabnet_oof_koleshjr_version2.csv\"\n\n##paths to the subs from the subsequent notebooks\nresnet1d_sub_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/resnet_1d_subs_and_oof/resnet1d_final_test_subs.csv\"\nfastai_tabular_sub_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/fastai_subs_and_oof/fastai_test_with_oof.csv\"\nlgbm_tabular_sub_path =\"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/lightgbm_subs_and_oof/lgb_test_with_oof.csv\"\nxgb_tabular_sub_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/xgb_subs_and_oof/xgb_test_with_oof.csv\"\ntabconv_sub_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/tabconv_subs_and_oof/tab_conv_test.csv\"\ntabtrans_sub_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/tabtransformer_subs_and_oof/transformer_tab_test.csv\"\nfastai_gated_sub_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/fastai_gated_cnn_subs_and_oof/fastai_Gated_CNN_test_with_oof.csv\"\nwavenet_gru_sub_path =\"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/wavenet_subs_and_oof/wavenet_gru_transformer_submission_version9.csv\"\ntabnet_sub_path = \"//kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/tabnet_subs_and_oof/tabnet_version2.csv\"\n\n\n#load the oof dfs\nlgbm_oof = pd.read_csv(lgbm_oof_path).sort_values(by=['event_t', 'location_id'])\nxgb_oof = pd.read_csv(xgb_oof_path).sort_values(by=['event_t', 'location_id'])\ntabconv_oof = pd.read_csv(tabconv_oof_path).sort_values(by=['event_t', 'location_id'])\ntabtrans_oof = pd.read_csv(tabtrans_oof_path).sort_values(by=['event_t', 'location_id'])\nfastai_gated_oof = pd.read_csv(fastai_gated_oof_path).sort_values(by=['event_t', 'location_id'])\nwavenet_gru_oof = pd.read_csv(wavenet_gru_oof_path).sort_values(by=['event_t', 'location_id'])\ntabnet_oof = pd.read_csv(tabnet_oof_path).sort_values(by=['event_t', 'location_id'])\nfastai_tabular_oof = pd.read_csv(fastai_tabular_oof_path).sort_values(by=['event_t', 'location_id'])\nresnet1d_oof = pd.read_csv(resnet1d_oof_path).sort_values(by=['event_t', 'location_id'])\n\n#load the sub dfs\nlgbm_sub= pd.read_csv(lgbm_tabular_sub_path).sort_values('event_id')\nxgb_sub = pd.read_csv(xgb_tabular_sub_path).sort_values('event_id')\ntabconv_sub = pd.read_csv(tabconv_sub_path).sort_values('event_id')\ntabtrans_sub = pd.read_csv(tabtrans_sub_path).sort_values('event_id')\nfastai_gated_sub = pd.read_csv(fastai_gated_sub_path).sort_values('event_id')\nwavenet_gru_sub = pd.read_csv(wavenet_gru_sub_path).sort_values('event_id')\ntabnet_sub = pd.read_csv(tabnet_sub_path).sort_values('event_id')\nfastai_tabular_sub = pd.read_csv(fastai_tabular_sub_path).sort_values('event_id')\nresnet1d_subs = pd.read_csv(resnet1d_sub_path).sort_values('event_id')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:41:45.666719Z","iopub.execute_input":"2025-02-18T10:41:45.667039Z","iopub.status.idle":"2025-02-18T10:42:00.856004Z","shell.execute_reply.started":"2025-02-18T10:41:45.667009Z","shell.execute_reply":"2025-02-18T10:42:00.854817Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Checking the individual oof of each model approach and the uniform ensemble","metadata":{}},{"cell_type":"code","source":"resnet1d_score = get_score(resnet1d_oof['resnet1d_oof_preds'].values, lgbm_oof['label'].values)\nprint(f\"resnet1d score: {resnet1d_score}\")\n\n# wavenet_gru_transformer_score = get_score(wavenet_gru_transformer_oof['predictions'].values, wavenet_gru_transformer_oof['label'].values)\n# print(f\"wavenet_gru_transformer score: {wavenet_gru_transformer_score}\")\n\n\nfastai_score = get_score(fastai_tabular_oof['oof_fastai'].values, fastai_tabular_oof['label'].values)\nprint(f\"fastai model score: {fastai_score}\")\n\n\nlgbm_score = get_score(lgbm_oof['oof_lgb'].values, lgbm_oof['label'].values)\nprint(f\"lgbm score: {lgbm_score}\")\n\nxgb_score = get_score(xgb_oof['oof_xgb'].values, xgb_oof['label'].values)\nprint(f\"xgb score: {xgb_score}\")\n\ntabconv_score = get_score(tabconv_oof['oof_preds'].values, tabconv_oof['label'].values)\nprint(f\"tabconv score: {tabconv_score}\")\n\ntabtrans_score = get_score(tabtrans_oof['oof_preds'].values, tabtrans_oof['label'].values)\nprint(f\"tabtrans score: {tabtrans_score}\")\n\nfastai_gated_score = get_score(fastai_gated_oof['oof_fastai'].values, fastai_gated_oof['label'].values)\nprint(f\"fastai_gated score: {fastai_gated_score}\")\n\nwavenet_gru_score = get_score(wavenet_gru_oof['wavelt_oof_preds'].values, wavenet_gru_oof['label'].values)\nprint(f\"wavenet_gru score: {wavenet_gru_score}\")\n\ntabnet_score = get_score(tabnet_oof['oof_tabnet_pred'].values, tabnet_oof['label'].values)\nprint(f\"tabnet score: {tabnet_score}\")\n\n\n\n#ensemble_oof = (lgbm_oof['oof_lgb'].values + ['predictions'].values + wavenet_gru_transformer_oof['predictions'].values + fastai_tabular_oof['oof_preds'].values) / 4\nensemble_oof = (lgbm_oof['oof_lgb'].values + xgb_oof['oof_xgb'].values + fastai_tabular_oof['oof_fastai'].values + tabconv_oof['oof_preds'] + tabtrans_oof['oof_preds'] +\n               fastai_gated_oof['oof_fastai'].values + wavenet_gru_oof['wavelt_oof_preds'].values + tabnet_oof['oof_tabnet_pred'].values + resnet1d_oof['resnet1d_oof_preds'].values)/9\nlgbm_oof['ensemble_oof'] = ensemble_oof\nuniform_ensemble_score = get_score(lgbm_oof['ensemble_oof'], lgbm_oof['label'].values)\nprint(f\"Uniform Ensemble score: {uniform_ensemble_score}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:42:00.857173Z","iopub.execute_input":"2025-02-18T10:42:00.857567Z","iopub.status.idle":"2025-02-18T10:42:02.207277Z","shell.execute_reply.started":"2025-02-18T10:42:00.857529Z","shell.execute_reply":"2025-02-18T10:42:02.206101Z"}},"outputs":[{"name":"stdout","text":"resnet1d score: 0.0024075282646260124\nfastai model score: 0.0026564100747232173\nlgbm score: 0.002392372256798666\nxgb score: 0.002344927985875686\ntabconv score: 0.002568253778283404\ntabtrans score: 0.002845684083937957\nfastai_gated score: 0.0026142450242668466\nwavenet_gru score: 0.0026192319050095864\ntabnet score: 0.002566036507378727\nUniform Ensemble score: 0.0020882354159607714\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Normalizing the Probabilities Based on Flood Probability\n* Legitimacy of the Step\n\nThis normalization is a valid preprocessing step because we are leveraging the flood probability feature predicted by our image classifier in the second stage. Since this feature is model-derived and not manually adjusted or externally injected(like how the leak used row order), it maintains the integrity of the pipeline.\n\n* Mathematical Justification\n\nNormalizing probabilities is a mathematically sound approach that aligns with the objective of minimizing log loss. Unlike setting hard thresholds or rounding values—both of which introduce artificial discontinuities—normalization preserves the relative ordering of predictions while adjusting them to be more coherent with an additional signal (flood probability). This transformation does not arbitrarily manipulate predictions but rather refines their distribution in a way that maintains probabilistic consistency.\n\n* Generalizability & Robustness\n\nThis method is inherently generalizable because it relies on a predictive feature (flood probability) rather than dataset-specific artifacts like row order(as seen in the discussion leak), which has been a concern in the discussions. By anchoring the adjustment to a meaningful feature rather than data leakage, we ensure that the approach remains effective across different splits and datasets.\n\n* Empirical Evidence\n\nThe improved training and test out-of-fold (OOF) scores confirm that this normalization enhances predictive performance. A technique that boosts both training and test scores while maintaining conceptual soundness is likely to generalize well beyond the current dataset.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\nprint(f\"logloss before normalizing: {log_loss(lgbm_oof['label'], lgbm_oof['ensemble_oof'])}\")\n\nlocations_to_normalize = lgbm_oof[lgbm_oof['flood_probability'] >= 0.7]['location_id'].unique()\nlgbm_oof['oof_sum_prob'] = lgbm_oof.groupby('location_id')['ensemble_oof'].transform('sum')\n\n# Avoid division by zero\nepsilon = 1e-8\nlgbm_oof['ensemble_oof_norm'] = lgbm_oof['ensemble_oof']  # Copy original values\n\nlgbm_oof.loc[lgbm_oof['location_id'].isin(locations_to_normalize), 'ensemble_oof_norm'] = (\n    lgbm_oof.loc[lgbm_oof['location_id'].isin(locations_to_normalize), 'ensemble_oof'] /\n    (lgbm_oof.loc[lgbm_oof['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n)\n\nprint(f\"logloss after normalizing: {log_loss(lgbm_oof['label'], lgbm_oof['ensemble_oof_norm'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:42:02.208318Z","iopub.execute_input":"2025-02-18T10:42:02.208605Z","iopub.status.idle":"2025-02-18T10:42:02.686197Z","shell.execute_reply.started":"2025-02-18T10:42:02.208581Z","shell.execute_reply":"2025-02-18T10:42:02.685083Z"}},"outputs":[{"name":"stdout","text":"logloss before normalizing: 0.0020882354159607714\nlogloss after normalizing: 0.0020165476689310603\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Optimizing the ensemble weights using nelder mead rather than using a uniform ensemble","metadata":{}},{"cell_type":"code","source":"# ens = [lgbm_oof['oof_pred'].values, wavenet_gru_transformer_oof['predictions'].values, resnet1d_oof['predictions'].values, fastai_tabular_oof['oof_preds'].values]\nens = [lgbm_oof['oof_lgb'].values,\n       xgb_oof['oof_xgb'].values,\n       fastai_tabular_oof['oof_fastai'].values,\n       tabconv_oof['oof_preds'].values,\n       tabtrans_oof['oof_preds'].values,\n       fastai_gated_oof['oof_fastai'].values ,\n       wavenet_gru_oof['wavelt_oof_preds'].values,\n       tabnet_oof['oof_tabnet_pred'].values,\n       resnet1d_oof['resnet1d_oof_preds'].values\n      ]\nlabels = lgbm_oof['label'].values\n\nn = len(ens)\ninitial_weights = [1/n]*n\n# Constraint: weights sum to 1\nconstraints = ({'type': 'eq', 'fun': lambda w: sum(w) - 1})\n\ndef objective(weights):\n    ensemble_preds = np.zeros(len(ens[0]))\n    for k in range(len(weights)):\n        ensemble_preds += weights[k]*ens[k]\n    loss = log_loss(labels, ensemble_preds)\n    return loss\n\ndef scipy_opt(ens,seed=42):  \n        def scorer_sc(w):  \n\n            labels = lgbm_oof['label'].values\n                \n            w = [ wi/(sum(w)) for wi in w ]  \n            \n            i=0\n            \n            wtd=None\n            \n            for _w in w: \n                \n                if i==0:\n                    \n                    wtd = ens[i] *_w\n                \n                else:\n                    \n                    wtd+=ens[i]*_w\n                \n                i=i+1  \n            \n            scr= get_score(wtd, labels)\n            \n            return scr\n        tol = 1e-10  \n        \n        total = len(ens)\n        \n        init_weights = [ 1/(total) for i in range(total) ] \n        \n        #print(init_weights)\n        \n        result = optimize.minimize(scorer_sc,\n                                        \n                                   init_weights,\n                                        \n                                   #constraints=({'type': 'eq','fun': lambda w: 1-sum(w),'jac': lambda x: [1] * len(x)}),\n                                        \n                                   method= 'Nelder-Mead',#'Nelder-Mead', #'SLSQP',\n                                        \n                                   bounds=[(0.001, 1.0)] * len(ens),\n                                   #options = {'ftol':1e-10},\n                                   #tol=tol\n                                   options={'disp': True} \n                                    \n                                  ) \n        \n        w = result['x']\n        \n        print(f'Optimum weights = {w} [{sum(w)}] \\n') \n        weights = [ wi/(sum(w)) for wi in w ] \n        print('With CV =',  result['fun'], \" to use \",weights ) \n        return  result['fun'],weights \n    \n    \n_,w=  scipy_opt(ens,seed=CFG.seed)     \nresult = optimize.minimize(objective, initial_weights, )\nw2 = result.x\nw2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:42:02.687493Z","iopub.execute_input":"2025-02-18T10:42:02.687939Z","iopub.status.idle":"2025-02-18T10:47:36.562072Z","shell.execute_reply.started":"2025-02-18T10:42:02.687901Z","shell.execute_reply":"2025-02-18T10:47:36.561015Z"}},"outputs":[{"name":"stdout","text":"Optimum weights = [0.00100071 0.75168319 0.50319245 0.33788598 0.2264627  0.19454701\n 0.3107063  0.99978013 0.56944328] [3.894701754292776] \n\nWith CV = 0.0020720855380638695  to use  [0.0002569420897732828, 0.19300147733824388, 0.12919922640441578, 0.08675528973463324, 0.05814635131465559, 0.049951709088699316, 0.079776660546064, 0.25670261549658385, 0.14620972798693113]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([0.10040714, 0.14227863, 0.13246341, 0.11017624, 0.06199604,\n       0.07317186, 0.11323705, 0.26546746, 0.16159635])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"ens = [lgbm_oof['oof_lgb'].values,\n       xgb_oof['oof_xgb'].values,\n       fastai_tabular_oof['oof_fastai'].values,\n       tabconv_oof['oof_preds'].values,\n       tabtrans_oof['oof_preds'].values,\n       fastai_gated_oof['oof_fastai'].values ,\n       wavenet_gru_oof['wavelt_oof_preds'].values,\n       tabnet_oof['oof_tabnet_pred'].values,\n       resnet1d_oof['resnet1d_oof_preds'].values\n      ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:47:36.564241Z","iopub.execute_input":"2025-02-18T10:47:36.564544Z","iopub.status.idle":"2025-02-18T10:47:36.569512Z","shell.execute_reply.started":"2025-02-18T10:47:36.564517Z","shell.execute_reply":"2025-02-18T10:47:36.568394Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"sample_submission = pd.read_csv(BASE_PATH + 'SampleSubmission.csv').sort_values('event_id')\nsample_submission['label'] = w2[0]*lgbm_sub['lgb_preds'].values + \\\n                             w2[1]*xgb_sub['xgb_preds'].values + \\\n                             w2[2]*fastai_tabular_sub['fastai_preds'].values + \\\n                             w2[3]*tabconv_sub['preds'].values + \\\n                             w2[4]*tabtrans_sub['preds'].values + \\\n                             w2[5]*fastai_gated_sub['fastai_preds'].values + \\\n                             w2[6]*wavenet_gru_sub['wavelt_preds'].values + \\\n                             w2[7]*tabnet_sub['tabnet_preds'].values + \\\n                             w2[8]*resnet1d_subs['resnet_sub_preds'].values\n\n\n\n\nsample_submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:34.532693Z","iopub.execute_input":"2025-02-18T10:51:34.533047Z","iopub.status.idle":"2025-02-18T10:51:34.859598Z","shell.execute_reply.started":"2025-02-18T10:51:34.533020Z","shell.execute_reply":"2025-02-18T10:51:34.858611Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                    event_id     label\n86140    id_066zz28m11mr_X_0  0.000046\n86141    id_066zz28m11mr_X_1  0.000035\n86150   id_066zz28m11mr_X_10  0.000034\n86240  id_066zz28m11mr_X_100  0.000048\n86241  id_066zz28m11mr_X_101  0.000034","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>86140</th>\n      <td>id_066zz28m11mr_X_0</td>\n      <td>0.000046</td>\n    </tr>\n    <tr>\n      <th>86141</th>\n      <td>id_066zz28m11mr_X_1</td>\n      <td>0.000035</td>\n    </tr>\n    <tr>\n      <th>86150</th>\n      <td>id_066zz28m11mr_X_10</td>\n      <td>0.000034</td>\n    </tr>\n    <tr>\n      <th>86240</th>\n      <td>id_066zz28m11mr_X_100</td>\n      <td>0.000048</td>\n    </tr>\n    <tr>\n      <th>86241</th>\n      <td>id_066zz28m11mr_X_101</td>\n      <td>0.000034</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"sample_submission.to_csv('ensemble_all_models.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:37.360783Z","iopub.execute_input":"2025-02-18T10:51:37.361189Z","iopub.status.idle":"2025-02-18T10:51:37.776398Z","shell.execute_reply.started":"2025-02-18T10:51:37.361144Z","shell.execute_reply":"2025-02-18T10:51:37.775307Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Normalizing the predictions based on the Flood probability\n* Reasons explained above","metadata":{}},{"cell_type":"code","source":"print(sample_submission.shape, lgbm_sub.shape)\nmod_sub = pd.merge(sample_submission,lgbm_sub[['event_id','location_id', 'flood_probability']], on='event_id', how='left' )\nprint(mod_sub.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:38.830286Z","iopub.execute_input":"2025-02-18T10:51:38.830624Z","iopub.status.idle":"2025-02-18T10:51:38.882185Z","shell.execute_reply.started":"2025-02-18T10:51:38.830598Z","shell.execute_reply":"2025-02-18T10:51:38.881042Z"}},"outputs":[{"name":"stdout","text":"(163520, 2) (163520, 6)\n(163520, 4)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"locations_to_normalize = mod_sub[mod_sub['flood_probability'] >= 0.7]['location_id'].unique()\nmod_sub['oof_sum_prob'] = mod_sub.groupby('location_id')['label'].transform('sum')\n\n# Avoid division by zero\nepsilon = 1e-8\nmod_sub['pred_norm'] = mod_sub['label']  # Copy original values\n\nmod_sub.loc[mod_sub['location_id'].isin(locations_to_normalize), 'pred_norm'] = (\n    mod_sub.loc[mod_sub['location_id'].isin(locations_to_normalize), 'label'] /\n    (mod_sub.loc[mod_sub['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n)\n\nmod_sub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:38.984344Z","iopub.execute_input":"2025-02-18T10:51:38.984683Z","iopub.status.idle":"2025-02-18T10:51:39.057721Z","shell.execute_reply.started":"2025-02-18T10:51:38.984657Z","shell.execute_reply":"2025-02-18T10:51:39.056587Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                event_id     label      location_id  flood_probability  \\\n0    id_066zz28m11mr_X_0  0.000046  id_066zz28m11mr           0.013285   \n1    id_066zz28m11mr_X_1  0.000035  id_066zz28m11mr           0.013285   \n2   id_066zz28m11mr_X_10  0.000034  id_066zz28m11mr           0.013285   \n3  id_066zz28m11mr_X_100  0.000048  id_066zz28m11mr           0.013285   \n4  id_066zz28m11mr_X_101  0.000034  id_066zz28m11mr           0.013285   \n\n   oof_sum_prob  pred_norm  \n0      0.050589   0.000046  \n1      0.050589   0.000035  \n2      0.050589   0.000034  \n3      0.050589   0.000048  \n4      0.050589   0.000034  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>event_id</th>\n      <th>label</th>\n      <th>location_id</th>\n      <th>flood_probability</th>\n      <th>oof_sum_prob</th>\n      <th>pred_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_066zz28m11mr_X_0</td>\n      <td>0.000046</td>\n      <td>id_066zz28m11mr</td>\n      <td>0.013285</td>\n      <td>0.050589</td>\n      <td>0.000046</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_066zz28m11mr_X_1</td>\n      <td>0.000035</td>\n      <td>id_066zz28m11mr</td>\n      <td>0.013285</td>\n      <td>0.050589</td>\n      <td>0.000035</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_066zz28m11mr_X_10</td>\n      <td>0.000034</td>\n      <td>id_066zz28m11mr</td>\n      <td>0.013285</td>\n      <td>0.050589</td>\n      <td>0.000034</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_066zz28m11mr_X_100</td>\n      <td>0.000048</td>\n      <td>id_066zz28m11mr</td>\n      <td>0.013285</td>\n      <td>0.050589</td>\n      <td>0.000048</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_066zz28m11mr_X_101</td>\n      <td>0.000034</td>\n      <td>id_066zz28m11mr</td>\n      <td>0.013285</td>\n      <td>0.050589</td>\n      <td>0.000034</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"mod_sub[['event_id', 'pred_norm']].to_csv('ensemble_all_models_post_processed.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:41.966775Z","iopub.execute_input":"2025-02-18T10:51:41.967158Z","iopub.status.idle":"2025-02-18T10:51:42.377453Z","shell.execute_reply.started":"2025-02-18T10:51:41.967129Z","shell.execute_reply":"2025-02-18T10:51:42.376380Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}