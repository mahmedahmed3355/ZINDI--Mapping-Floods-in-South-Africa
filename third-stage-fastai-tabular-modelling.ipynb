{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":10776345,"datasetId":6682996,"databundleVersionId":11132163}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastai -q","metadata":{"id":"9i2iO3YCvV1Y","outputId":"9c662ea4-d059-4b99-f1ef-9be40520c3ff","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\nfrom sklearn.metrics import log_loss, roc_auc_score\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom itertools import combinations\nimport random\n\nfrom PIL import Image\n\n\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nimport gc\n\nimport torch\nfrom fastai.tabular.all import *\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n","metadata":{"id":"xvXbKgvd1dDh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"\nadditional_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/image_classifier_results/\"\ntrain = pd.read_csv(base_path + \"Train.csv\")\ntest = pd.read_csv(base_path + \"Test.csv\")\ntrain_with_cv_results = pd.read_csv(additional_path + \"train_with_cv_results.csv\")[['location_id', 'flood_probability']]\ntest_with_cv_results = pd.read_csv(additional_path + \"test_with_cv_results.csv\")[['location_id', 'flood_probability',]]\nsubmission = pd.read_csv(base_path + \"SampleSubmission.csv\")\nimages = np.load(base_path + \"composite_images.npz\")\ndisplay(train.head(), train.shape, train_with_cv_results.head(), train_with_cv_results.shape, test.head(), test.shape)\n","metadata":{"id":"r0jMp99i1zSE","outputId":"7d5265bf-2199-46f0-c2ad-ad02b0603997","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List all keys in the .npz file\nprint(\"Keys in the .npz file:\", images.files)\n\n# Access the first image array (adjust the key name as needed)\nkey_name = images.files[0]  # Replace with the correct key if needed\nimage_array = images[key_name]\n\n# Print the shape of the image\nprint(\"Shape of the image array:\", image_array.shape)\n\n# Extract the number of bands (assumes shape is (height, width, bands))\nif image_array.ndim == 3:\n    num_bands = image_array.shape[2]\n    print(\"Number of bands in the image:\", num_bands)\nelse:\n    print(\"The image array does not have multiple bands (2D array).\")","metadata":{"id":"c-Gs4LTb2z_F","outputId":"389c0019-fdf5-4704-ca1a-87878ff435a3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_location(value):\n  return value.split(\"_\")[0] + '_' + value.split(\"_\")[1]\n\ndef get_event_id(value):\n  return value.split(\"_\")[3]\nfor df in [train, test]:\n\n  df['location_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n  df['event_idx'] = df.groupby('location_id', sort=False).ngroup()\n\n  df['event_t'] = df.groupby('location_id').cumcount()\n\nprint(len(set(train['location_id'])), len(set(test['location_id'])))\nprint(len(set(train['location_id']).intersection(set(test['location_id']))))\nprint(len(images))\ndisplay(train.head(), test.head())","metadata":{"id":"3gSThGFbHugs","outputId":"4a39ac63-442b-449d-a6bc-4de5e4642ff5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* each image has 730 events\n* no intersection of images betweeen the two data sets (unique sets)\n* The numpy files has 898 images for both train and test\n\n* The images are annual cloud-free composite images from Sentinel-2 satellite imagery. They are of size 128x128 and contain the following 6 channels:\n\n      Sentinel-2 B2 (Blue)\n      Sentinel-2 B3 (Green)\n      Sentinel-2 B4 (Red)\n      Sentinel-2 B8 (NIR)\n      Sentinel-2 B11 (SWIR)\n      Slope (derived from NASA SRTM)\n\n* the images are essentially static for any event/location pair over the study period.\n  * the images only serve as spatial representations of the environment for that location over the 730 day period\n  * it reflects static or semi-static environmental conditons (e.g land use, vegetation, water bodies, topography) that could influence flood occurence\n  * so the images cannot provide temporal insights but what we can do is extract spatial features such as NDVI, NDWI, NDBI, Topographic features like slope and elevation changes from the slope channel\n  * combine the spatial features with temporal precipitation data to enrich the dataset by treating the spatial features as fixed covariates that describe each location.\n    * Areas with high NDWI Might flood more frequently with heavy precipitation\n    * LOcations with high slope values might experience flash floods after intense rainfall\n\n  * Image processing:\n    * Use pretrained models to extract image embeddings or use PCA for dimensionality reduction\n    * create a binary classifier where 1 is images where a flood has occured in any of the 730 events and 0 if no floods has occured to create a soft flag for flood-prone locations. Even if not perfect they can serve as a proxy for environmental vulnerability to floods\n    * The image classifier naturally reduces the extreme imbalance in the dataset by focusing on binary flood/non-flood classification\n  \n  * clustering locations:\n    * group events/locations based on spatial features (e.g NDVI, NDWI) to identify patterns in flood susceptibility\n  * correlating spatial features with precipitation thresholds:\n    * study how spatial features interact with specific precipitation thresholds that leads to floods\n\n  * You can think of event_id_X_1 being the 01/01/2024 and event_id_X_2 being 02/01/2024 (dd/mm/yyyy).","metadata":{"id":"X5CuKrF84NhH"}},{"cell_type":"markdown","source":"### Data Preprocessing + Feature Engineering","metadata":{"id":"AJtxRMgI8AP8"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.ndimage import uniform_filter\n\ndef calculate_statistics(array, stats):\n    \"\"\"\n    Compute specified statistics on a given array.\n\n    Args:\n        array (numpy.ndarray): Input array.\n        stats (list): List of statistics to calculate (e.g., ['mean', 'std', 'median']).\n\n    Returns:\n        dict: Dictionary of computed statistics with keys as stat names.\n    \"\"\"\n    statistics = {}\n    for stat in stats:\n        if stat == 'mean':\n            statistics['mean'] = np.nanmean(array)\n        elif stat == 'std':\n            statistics['std'] = np.nanstd(array)\n        elif stat == 'median':\n            statistics['median'] = np.nanmedian(array)\n        elif stat == 'skew':\n            statistics['skew'] = pd.Series(array.flatten()).skew()\n        elif stat == 'min':\n            statistics['min'] = np.nanmin(array)\n        elif stat == 'max':\n            statistics['max'] = np.nanmax(array)\n    return statistics\n\n\ndef generate_flood_features(df, images, band_names, statistics=['mean', 'std']):\n    \"\"\"\n    Generate tabular features for flood modeling from given bands, with customizable statistics.\n\n    Args:\n        df (pd.DataFrame): Input dataframe containing metadata for each location (train + test).\n                           Should have a 'location_id' column to identify images.\n        images (dict): Dictionary of image data indexed by 'location_id'.\n        band_names (tuple): Tuple of available band names (e.g., 'B2', 'B3', ...).\n        statistics (list): List of statistics to calculate for each feature (e.g., ['mean', 'std']).\n\n    Returns:\n        pd.DataFrame: Dataframe with additional features for modeling.\n    \"\"\"\n    band_indices = {name: band_names.index(name) for name in band_names}\n    features = []\n\n    for _, row in df.iterrows():\n        location_id = row['location_id']\n        bands = {name: images[location_id][..., idx] for name, idx in band_indices.items()}\n\n        # Vegetation Indices\n        ndvi = (bands['B8'] - bands['B4']) / (bands['B8'] + bands['B4'] + 1e-6)\n        # savi = (1.5) * (bands['B8'] - bands['B4']) / (bands['B8'] + bands['B4'] + 0.5 + 1e-6)\n\n        # Water Indices\n        ndwi = (bands['B3'] - bands['B8']) / (bands['B3'] + bands['B8'] + 1e-6)\n        mndwi = (bands['B3'] - bands['B11']) / (bands['B3'] + bands['B11'] + 1e-6)\n\n        # Moisture-related\n        msi = bands['B11'] / (bands['B8'] + 1e-6)\n\n        # Topographic Features\n        slope = bands['SLOPE']\n        # slope_variability = uniform_filter(slope, size=3)  # Smooth with a 3x3 window\n        # elevation_change = np.gradient(slope, axis=(0, 1))\n\n        # # Composite Ratios\n        # band_ratios = [\n        #     (bands['B2'] / (bands['B4'] + 1e-6)),\n        #     (bands['B8'] / (bands['B11'] + 1e-6)),\n        #     (bands['B2'] / (bands['B8'] + 1e-6)),\n        # ]\n\n        # Compile features for this location\n        location_features = {'location_id': location_id}\n\n        # Add statistics for each feature\n        index_features = {\n            'NDVI': ndvi,\n            # 'SAVI': savi,\n            'NDWI': ndwi,\n            'MNDWI': mndwi,\n            'MSI': msi,\n            'Slope': slope,\n            # 'Slope_variability': slope_variability,\n            # 'Elevation_change': elevation_change,\n        }\n\n        for feature_name, feature_array in index_features.items():\n            stats = calculate_statistics(feature_array, statistics)\n            for stat_name, value in stats.items():\n                location_features[f'{feature_name}_{stat_name}'] = value\n\n        # # Add statistics for band ratios\n        # for idx, ratio in enumerate(band_ratios):\n        #     stats = calculate_statistics(ratio, statistics)\n        #     for stat_name, value in stats.items():\n        #         location_features[f'Band_Ratio_{idx + 1}_{stat_name}'] = value\n\n        features.append(location_features)\n\n    # Convert to a DataFrame\n    feature_df = pd.DataFrame(features)\n\n    # Merge with the original DataFrame (optional)\n    df = df.merge(feature_df, on='location_id', how='left')\n\n    return df\n\n# Example list of statistics to calculate\nstats_to_calculate = ['mean', 'median', 'std']#, 'std', 'median', 'min', 'max'\nBAND_NAMES = ('B2', 'B3', 'B4', 'B8', 'B11', 'SLOPE')\n\n# Generate features with these statistics\ntrain_features = generate_flood_features(train_with_cv_results, images, BAND_NAMES, statistics=stats_to_calculate)\ntest_features = generate_flood_features(test_with_cv_results, images, BAND_NAMES, statistics=stats_to_calculate)\n\ndisplay(train_features.head(), train_features.shape)","metadata":{"id":"hF3dyo-K6ys0","outputId":"49882b90-e24b-44d2-9e8d-71476716da04","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.merge(train, train_features, on='location_id', how='left')\ntest_df = pd.merge(test, test_features, on='location_id', how='left')\n\n\ndisplay(train_df.head(), train_df.shape, test_df.head(), test_df.shape)","metadata":{"id":"KZ9xoR3otFXL","outputId":"f880f8a9-0431-4151-bd58-4474efd12ac2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### More Feature Engineering","metadata":{"id":"2-LUpXHpEiI_"}},{"cell_type":"code","source":"from types import new_class\ndef apply_expanding_combinations(df, group_cols_list, target_col='Sales', shift_periods=[1], min_periods=1, stats=['mean', 'std']):\n    # Loop through the group column combinations\n    for group_cols in group_cols_list:\n        # Generate base name for the grouping\n        group_name = '_'.join(group_cols)\n\n        for shift_period in shift_periods:\n            for stat in stats:\n                expanding_col_name = f'expanding_grouped_{group_name}_{target_col}_shift_{shift_period}_{stat}'\n\n                # Apply groupby, shift, and expanding for the given statistic\n                df[expanding_col_name] = df.groupby(group_cols)[target_col].transform(\n                    lambda x: x.shift(shift_period).expanding(min_periods=min_periods).agg(stat)\n                )\n\n    return df\n\ndef smoothen_target(df, group_cols, target_col):\n  n_std = 10\n  for i_smooth in [target_col]:\n      df_id_outlier = df.groupby(group_cols,as_index=False).agg({\n          f'{i_smooth}': lambda x: x.mean() + n_std*x.std()\n      }).rename(columns={f'{i_smooth}':f'{i_smooth}_outlier'})\n\n      df_id_mean = df.groupby(group_cols,as_index=False).agg({\n          f'{i_smooth}': 'mean'\n      }).rename(columns={f'{i_smooth}':f'{i_smooth}_mean'})\n\n      df = df.merge(df_id_outlier, on=group_cols[0], how='left')\n      df = df.merge(df_id_mean, on=group_cols[0], how='left')\n\n      df[f'{i_smooth}'] = np.where(\n          df[f'{i_smooth}'] > df[f'{i_smooth}_outlier'],\n          df[f'{i_smooth}_mean'],\n          df[f'{i_smooth}']\n      )\n\n  return df\n\n\ndef create_rolling_features(data, group_cols, target_col, windows, shift_period, min_period, statistics):\n    def apply_statistic(x, stat):\n        rolled = x.shift(shift_period).rolling(window=window, min_periods=min_period)\n        if stat == 'mean':\n            return rolled.mean()\n        elif stat == 'median':\n            return rolled.median()\n        elif stat == 'std':\n            return rolled.std()\n        elif stat == 'min':\n            return rolled.min()\n        elif stat == 'max':\n            return rolled.max()\n        elif stat == 'skew':\n            return rolled.skew()\n        elif stat == 'sum':\n            return rolled.sum()\n        elif stat == 'quantile':\n            return rolled.quantile(0.95)\n\n        elif stat.startswith('quantile_'):\n            q = float(stat.split('_')[1])\n            return rolled.quantile(q)\n        else:\n            raise ValueError(f\"Unknown statistic: {stat}\")\n\n    for window in windows:\n        for stat in statistics:\n            stat_name = stat if not stat.startswith('quantile_') else f\"{stat.split('_')[1]}th\"\n            col_name = f'rolling_previous_grouped_{target_col}_{stat_name}_{window}_{shift_period}'\n\n            data[col_name] = data.groupby(group_cols)[target_col].transform(\n                lambda x: apply_statistic(x, stat)\n            )\n\n    return data\n\n\n\ndef custom_agg(x):\n    return x.max() - x.min()\n\ndef get_date_features(df):\n  # Simulate year (assuming 365 days per year)\n  df['year'] = (df['event_t'] // 365) + 1  # Year 1 or 2\n\n  # Simulate month (approximate)\n  df['month'] = ((df['event_t'] % 365) // 30) + 1  # 30-day months approximation\n\n  # Simulate week of the year\n  df['week_of_year'] = (df['event_t'] % 365) // 7 + 1\n\n  # Simulate day of the month\n  df['day_of_month'] = (df['event_t'] % 30) + 1  # Assuming 30-day months\n\n  # Simulate day of the week (0 = Monday, 6 = Sunday)\n  df['day_of_week'] = df['event_t'] % 7\n\n  # Simulate quarter\n  df['quarter'] = ((df['month'] - 1) // 3) + 1\n  return  df\n\n\n\n\ndef feature_engineering(train, test):\n  data = pd.concat([train, test])\n  data.sort_values(by = ['location_id', 'event_t'], inplace=True)\n  data['event_t'] = data['event_t'].astype(int)\n  # data = smoothen_target(data, ['location_id'], 'precipitation')\n\n  data['event_binary'] = data['event_t'].apply(lambda x: 1 if (x >= 296 and x <= 435) else 0)\n\n\n  group_cols =['location_id']\n  # data = apply_expanding_combinations(\n  #     data,\n  #     [group_cols],\n  #     target_col='precipitation',\n  #     shift_periods=[1],#1,3, 4, 5, 6, 7, 8, 24\n  #     min_periods=1,\n  #     stats=['mean']\n  # )\n\n  statistics = ['mean'] #, 'median', 'std', 'quantile_0.25', 'quantile_0.75'\n  min_period = 1\n\n  shift_period = 0\n  windows = [3, 4,10,20, 25, 30,55,60, 75, 296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n\n  # shift_period = 2\n  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n\n  # shift_period = 3\n  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n\n  # shift_period = 4\n  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n\n  # shift_period = 5\n  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n\n  # shift_period = 6\n  # windows = [ 3,4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n\n  # shift_period = 8\n  # windows = [ 3,4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n  # # data = get_date_features(data)\n\n  for col in ['precipitation']:\n    # data[f\"grouped_location_{col}_cum\"] = data.groupby('location_id')[col].cumsum().shift(1)\n\n    # quantile = 0.95  # Define the quantile you want to calculate\n    # for stat in ['mean', 'quantile']:\n    #     if stat != 'quantile':\n    #         data[f\"location_grouped_{col}_{stat}\"] = data.groupby('location_id')[col].transform(stat)\n    #         data[f\"diff_{col}_{stat}\"] = data[col] - data[f\"location_grouped_{col}_{stat}\"]\n\n\n    for shift in range(1,365):\n      data[f'{col}_shift_{shift}'] = data.groupby('location_id')[col].shift(shift)\n      data[f'{col}_next_shift_{shift}'] = data.groupby('location_id')[col].shift(-shift)\n\n\n\n\n    # for window in windows:\n    #   data[f'{col}_rolling_grouped_custom_{window}'] = (\n    #       data.groupby('location_id')[col]\n    #       .rolling(window)\n    #       .apply(custom_agg)\n    #       .reset_index(level=0, drop=True)  # Reset the index to align with the original DataFrame\n    #   )\n\n    for span in [7]:\n        data[f'{col}_ewm_grouped_mean_{span}'] = (\n            data.groupby('location_id')[col]\n            .ewm(span=span, adjust=False)\n            .mean()\n            .reset_index(level=0, drop=True)  # Reset the index to align it with the original DataFrame\n        )\n\n\n\n\n  train = data[data['label'].notna()].reset_index(drop = True)\n  test = data[data['label'].isna()].reset_index(drop = True)\n\n  return train, test\n\nnew_train, new_test = feature_engineering(train_df, test_df)\ndisplay(new_train.head(), new_train.shape, new_test.head(), new_test.shape)","metadata":{"id":"H-HFjk91EmcZ","outputId":"547845be-3658-4eb9-e62d-a9b6498c8b98","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MODELLING\n674 224","metadata":{"id":"7I1N0hGDlPHX"}},{"cell_type":"code","source":"new_train['label'].value_counts()","metadata":{"id":"6n4Fco4Yt0H0","outputId":"2ce44227-2c8a-4956-dc65-db23a8fea433","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nn_splits = 10\nseed = 2024\ngkf = StratifiedGroupKFold(n_splits = n_splits)\n\nnew_train['fold'] = -1\nfor fold, (_, val_idx) in enumerate(gkf.split(new_train, new_train['label'], groups = new_train['location_id'])):\n    new_train.loc[val_idx, \"fold\"] = fold\n# new_train['fold'] = new_train['fold'].astype(int)\nnew_train['fold'].value_counts()\n\n","metadata":{"id":"imSZVxb7lQZk","outputId":"41168ffa-9906-47f4-f244-839f2c0d495c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(n_splits):\n  print(new_train[new_train['fold'] == i]['label'].value_counts())\n  print(\"-\"* 100)","metadata":{"id":"m3E9y965uGMG","outputId":"45c607b0-cd43-40a9-ca39-a7a336aea025","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MODELLING","metadata":{"id":"PY2g-4yyuhO3"}},{"cell_type":"code","source":"\nindices_cols = [\n  'EVI_mean',\n 'EVI_median',\n 'EVI_std',\n 'MNDWI_mean',\n 'MNDWI_median',\n 'MNDWI_std',\n 'MSI_mean',\n 'MSI_median',\n 'MSI_std',\n 'NDVI_mean',\n 'NDVI_median',\n 'NDVI_std',\n 'NDWI_mean',\n 'NDWI_median',\n 'NDWI_std',\n 'Slope_mean',\n 'Slope_median',\n 'Slope_std',\n]\n\nselected_columns =['precipitation','flood_probability','event_binary', 'event_t', ] + [col for col in new_train if 'diff' in col or 'shift' in col or 'grouped' in col ]\n\nprint(selected_columns)\ntarget_col = 'label'\n","metadata":{"id":"MmKNl_CWz8B_","outputId":"4e0c230e-5a87-457e-d03a-3536e9602863","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value)\n #cpu vars\n    torch.manual_seed(seed_value)\n# cpu  vars\n    random.seed(seed_value)\n # Python\n    if use_cuda:\n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n# gpu vars\n        torch.backends.cudnn.deterministic = True\n #needed\n        torch.backends.cudnn.benchmark = False\n#Remember to use num_workers=0 when creating the DataBunch.\n\nrandom_seed(2024,True)\n\ndef fit_fastai_model(train, test, target_col, selected_columns, n_splits):\n    train['oof_preds'] = 0.0  # Initialize OOF predictions in train\n    test_preds = np.zeros(len(test), dtype=np.float32)  # Initialize test predictions\n    scores_auc = []  # Store AUC scores\n    scores_logloss = []  # Store Log Loss scores\n\n    cat_feats = []  # Categorical features\n    cont_feats = [col for col in selected_columns if col not in cat_feats]  # Continuous features\n\n    for fold in range(n_splits):\n        print(\"*\" * 100)\n        print(f\"======================================TRAINING FOLD: {fold}=============================================\")\n\n        # Split train into training and validation sets\n        training = train[train['fold'] != fold]\n        validation = train[train['fold'] == fold]\n\n        splits = (\n            list(range(len(training))),\n            list(range(len(training), len(training) + len(validation)))\n        )\n\n        combined_data = pd.concat(\n            [training[selected_columns + [target_col]], validation[selected_columns + [target_col]]]\n        )\n\n        # Prepare DataLoaders\n        dls = TabularPandas(\n            combined_data,\n            cat_names=cat_feats,\n            cont_names=cont_feats,\n            y_names=target_col,\n            splits=splits,\n            procs=[Categorify, FillMissing, Normalize]\n        ).dataloaders(bs=4096)\n\n        # Define the model as a binary classifier\n        learn = tabular_learner(\n            dls,\n            layers=[ 256,512, 1024, 512, 256],\n            n_out=1,\n            loss_func=F.binary_cross_entropy_with_logits,\n            metrics=[AccumMetric(roc_auc_score, invert_arg=True)]\n        )\n\n        # Train the model\n        learn.fit_one_cycle(10, 2e-3, cbs=[SaveModelCallback(monitor='valid_loss', fname=f'nn_approach_fold_{fold}')])\n\n        # Validation predictions\n        val_dl = learn.dls.test_dl(validation[selected_columns])\n        preds, _ = learn.get_preds(dl=val_dl)\n        val_preds = preds.sigmoid().squeeze().numpy()  # Sigmoid for probabilities\n        auc_score = roc_auc_score(validation[target_col], val_preds)\n        logloss_score = log_loss(validation[target_col], val_preds)\n\n        scores_auc.append(auc_score)\n        scores_logloss.append(logloss_score)\n\n        print(f\"Fold {fold} AUC: {auc_score:.4f}, LogLoss: {logloss_score:.4f}\")\n\n        # Assign OOF predictions to train\n        train.loc[validation.index, 'oof_fastai'] = val_preds\n\n        # Test predictions\n        test_dl = learn.dls.test_dl(test[selected_columns])\n        preds, _ = learn.get_preds(dl=test_dl)\n        test_preds += preds.sigmoid().squeeze().numpy()\n\n        # Cleanup\n        del dls, learn, val_dl, test_dl, preds, _\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    # Combine test predictions (mean across folds)\n    combined_test_preds = test_preds / n_splits\n\n    print(f\"\\nAverage AUC across {n_splits} folds: {np.mean(scores_auc):.4f} (+/- {np.std(scores_auc):.4f})\")\n    print(f\"Average LogLoss across {n_splits} folds: {np.mean(scores_logloss):.4f} (+/- {np.std(scores_logloss):.4f})\")\n\n    # Overall scores for OOF\n    overall_auc = roc_auc_score(train[target_col], train['oof_fastai'])\n    overall_logloss = log_loss(train[target_col], train['oof_fastai'])\n    print(f\"Overall OOF AUC: {overall_auc:.4f}, LogLoss: {overall_logloss:.4f}\")\n\n    # Assign combined test predictions to the test set\n    test['fastai_preds'] = combined_test_preds\n\n    return train, test\n","metadata":{"id":"3WmLackNV56Y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub_train, sub_test =  fit_fastai_model(new_train, new_test, target_col, selected_columns, n_splits)","metadata":{"id":"fSmavMp9uOS6","outputId":"70ad84e7-313f-47a7-d522-34d70627b93e","trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nAverage AUC across 10 folds: 0.9386 (+/- 0.0274)\nAverage LogLoss across 10 folds: 0.0027 (+/- 0.0003)\nOverall OOF AUC: 0.9310, LogLoss: 0.0027","metadata":{"id":"xG7rLk_mCbcf"}},{"cell_type":"code","source":"display(sub_test.head(), sub_test.shape)","metadata":{"id":"hm5njpHvyLd0","outputId":"92500447-8c4a-4569-d4c8-57bcc6ea5f8b","trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nsub = sub_test[['event_id', 'fastai_preds']]\nsub.head()","metadata":{"id":"v5sOx64wDQDj","outputId":"52ad3b13-5b25-49ae-cabd-09ee75daf2e2","trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub.to_csv(\"baseline_fastai_0.0027_10_folds_v2.csv\", index = False)","metadata":{"id":"AElrwDiJDrol","trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Normalizing the Probabilities","metadata":{"id":"PoL0_UhtcFNl"}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\nprint(f\"logloss before normalizing: {log_loss(sub_train['label'], sub_train['oof_fastai'])}\")\n\nlocations_to_normalize = sub_train[sub_train['flood_probability'] >= 0.5]['location_id'].unique()\nsub_train['oof_sum_prob'] = sub_train.groupby('location_id')['oof_fastai'].transform('sum')\n\n# Avoid division by zero\nepsilon = 1e-8\nsub_train['oof_fastai_norm'] = sub_train['oof_fastai']  # Copy original values\n\nsub_train.loc[sub_train['location_id'].isin(locations_to_normalize), 'oof_fastai_norm'] = (\n    sub_train.loc[sub_train['location_id'].isin(locations_to_normalize), 'oof_fastai'] /\n    (sub_train.loc[sub_train['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n)\n\nprint(f\"logloss after normalizing: {log_loss(sub_train['label'], sub_train['oof_fastai_norm'])}\")\n","metadata":{"id":"_joHnyu5fajH","outputId":"185ee874-9052-4116-c93f-e3e3c020fff6","trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub_train[['event_id', 'location_id', 'event_t', 'flood_probability','label','oof_fastai', ]].to_csv(\"fastai_train_with_oof.csv\", index=False)\nsub_test[['event_id', 'location_id', 'event_t', 'flood_probability','label', 'fastai_preds']].to_csv(\"fastai_test_with_oof.csv\", index=False)","metadata":{"id":"6CYjO0ZpBsfG","trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.915Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Normalize the predictions based on the flood probability\n","metadata":{}},{"cell_type":"code","source":"locations_to_normalize = sub_test[sub_test['flood_probability'] >= 0.7]['location_id'].unique()\nsub_test['oof_sum_prob'] = sub_test.groupby('location_id')['fastai_preds'].transform('sum')\n\n# Avoid division by zero\nepsilon = 1e-8\nsub_test['pred_norm'] = sub_test['fastai_preds']  # Copy original values\n\nsub_test.loc[sub_test['location_id'].isin(locations_to_normalize), 'pred_norm'] = (\n    sub_test.loc[sub_test['location_id'].isin(locations_to_normalize), 'fastai_preds'] /\n    (sub_test.loc[sub_test['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n)\n\nsub_test.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmod_sub = sub_test[['event_id', 'pred_norm']]\nmod_sub.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mod_sub.to_csv(\"fastai_mod_0_25(corrected).csv\", index = False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-16T00:18:45.917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}