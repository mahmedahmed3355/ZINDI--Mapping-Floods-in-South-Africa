{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10716224,
          "sourceType": "datasetVersion",
          "datasetId": 6642380
        },
        {
          "sourceId": 10723932,
          "sourceType": "datasetVersion",
          "datasetId": 6220790
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install pytorch-tabnet"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T16:41:00.477390Z",
          "iopub.execute_input": "2025-02-17T16:41:00.477631Z",
          "iopub.status.idle": "2025-02-17T16:41:06.294755Z",
          "shell.execute_reply.started": "2025-02-17T16:41:00.477609Z",
          "shell.execute_reply": "2025-02-17T16:41:06.293617Z"
        },
        "id": "SvoE7vsnimJw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
        "from pytorch_tabnet.metrics import Metric\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "\n",
        "from itertools import combinations\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import catboost as catt\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "VERSION = 2"
      ],
      "metadata": {
        "id": "xvXbKgvd1dDh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T16:41:33.238600Z",
          "iopub.execute_input": "2025-02-17T16:41:33.239004Z",
          "iopub.status.idle": "2025-02-17T16:41:39.734974Z",
          "shell.execute_reply.started": "2025-02-17T16:41:33.238970Z",
          "shell.execute_reply": "2025-02-17T16:41:39.733916Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"\n",
        "additional_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/image_classifier_results/\"\n",
        "train = pd.read_csv(base_path + \"Train.csv\")\n",
        "test = pd.read_csv(base_path + \"Test.csv\")\n",
        "train_with_cv_results = pd.read_csv(additional_path + \"train_with_cv_results.csv\")[['location_id', 'flood_probability']]\n",
        "test_with_cv_results = pd.read_csv(additional_path + \"test_with_cv_results.csv\")[['location_id', 'flood_probability',]]\n",
        "submission = pd.read_csv(base_path + \"SampleSubmission.csv\")\n",
        "images = np.load(base_path + \"composite_images.npz\")\n",
        "display(train.head(), train.shape, train_with_cv_results.head(), train_with_cv_results.shape, test.head(), test.shape)"
      ],
      "metadata": {
        "id": "r0jMp99i1zSE",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T16:43:00.783228Z",
          "iopub.execute_input": "2025-02-17T16:43:00.783566Z",
          "iopub.status.idle": "2025-02-17T16:43:01.560613Z",
          "shell.execute_reply.started": "2025-02-17T16:43:00.783540Z",
          "shell.execute_reply": "2025-02-17T16:43:01.559643Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['label']== 1].describe()"
      ],
      "metadata": {
        "id": "72oZ30oVOBT1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:08.890751Z",
          "iopub.execute_input": "2025-02-16T02:23:08.891052Z",
          "iopub.status.idle": "2025-02-16T02:23:08.907828Z",
          "shell.execute_reply.started": "2025-02-16T02:23:08.891028Z",
          "shell.execute_reply": "2025-02-16T02:23:08.907012Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['label']== 0].describe()"
      ],
      "metadata": {
        "id": "bBEPGv4hOIL2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:08.908725Z",
          "iopub.execute_input": "2025-02-16T02:23:08.909047Z",
          "iopub.status.idle": "2025-02-16T02:23:08.965774Z",
          "shell.execute_reply.started": "2025-02-16T02:23:08.909016Z",
          "shell.execute_reply": "2025-02-16T02:23:08.965016Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_location(value):\n",
        "  return value.split(\"_\")[0] + '_' + value.split(\"_\")[1]\n",
        "\n",
        "def get_event_id(value):\n",
        "  return value.split(\"_\")[3]\n",
        "for df in [train, test]:\n",
        "\n",
        "  df['location_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
        "  df['event_idx'] = df.groupby('location_id', sort=False).ngroup()\n",
        "\n",
        "  df['event_t'] = df.groupby('location_id').cumcount()\n",
        "\n",
        "print(len(set(train['location_id'])), len(set(test['location_id'])))\n",
        "print(len(set(train['location_id']).intersection(set(test['location_id']))))\n",
        "display(train.head(), test.head())"
      ],
      "metadata": {
        "id": "c-Gs4LTb2z_F",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:08.966548Z",
          "iopub.execute_input": "2025-02-16T02:23:08.966794Z",
          "iopub.status.idle": "2025-02-16T02:23:09.503611Z",
          "shell.execute_reply.started": "2025-02-16T02:23:08.966773Z",
          "shell.execute_reply": "2025-02-16T02:23:09.502924Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train.groupby(['location_id'])['event_id'].count()"
      ],
      "metadata": {
        "id": "PQe32657iK8y",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:09.504367Z",
          "iopub.execute_input": "2025-02-16T02:23:09.504580Z",
          "iopub.status.idle": "2025-02-16T02:23:09.565077Z",
          "shell.execute_reply.started": "2025-02-16T02:23:09.504562Z",
          "shell.execute_reply": "2025-02-16T02:23:09.564266Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "* each image has 730 events\n",
        "* no intersection of images betweeen the two data sets (unique sets)\n",
        "* The numpy files has 898 images for both train and test\n",
        "\n",
        "* The images are annual cloud-free composite images from Sentinel-2 satellite imagery. They are of size 128x128 and contain the following 6 channels:\n",
        "\n",
        "      Sentinel-2 B2 (Blue)\n",
        "      Sentinel-2 B3 (Green)\n",
        "      Sentinel-2 B4 (Red)\n",
        "      Sentinel-2 B8 (NIR)\n",
        "      Sentinel-2 B11 (SWIR)\n",
        "      Slope (derived from NASA SRTM)\n",
        "\n",
        "* the images are essentially static for any event/location pair over the study period.\n",
        "  * the images only serve as spatial representations of the environment for that location over the 730 day period\n",
        "  * it reflects static or semi-static environmental conditons (e.g land use, vegetation, water bodies, topography) that could influence flood occurence\n",
        "  * so the images cannot provide temporal insights but what we can do is extract spatial features such as NDVI, NDWI, NDBI, Topographic features like slope and elevation changes from the slope channel\n",
        "  * combine the spatial features with temporal precipitation data to enrich the dataset by treating the spatial features as fixed covariates that describe each location.\n",
        "    * Areas with high NDWI Might flood more frequently with heavy precipitation\n",
        "    * LOcations with high slope values might experience flash floods after intense rainfall\n",
        "\n",
        "  * Image processing:\n",
        "    * Use pretrained models to extract image embeddings or use PCA for dimensionality reduction\n",
        "    * create a binary classifier where 1 is images where a flood has occured in any of the 730 events and 0 if no floods has occured to create a soft flag for flood-prone locations. Even if not perfect they can serve as a proxy for environmental vulnerability to floods\n",
        "    * The image classifier naturally reduces the extreme imbalance in the dataset by focusing on binary flood/non-flood classification\n",
        "  \n",
        "  * clustering locations:\n",
        "    * group events/locations based on spatial features (e.g NDVI, NDWI) to identify patterns in flood susceptibility\n",
        "  * correlating spatial features with precipitation thresholds:\n",
        "    * study how spatial features interact with specific precipitation thresholds that leads to floods\n",
        "\n",
        "  * You can think of event_id_X_1 being the 01/01/2024 and event_id_X_2 being 02/01/2024 (dd/mm/yyyy)."
      ],
      "metadata": {
        "id": "X5CuKrF84NhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing + Feature Engineering"
      ],
      "metadata": {
        "id": "AJtxRMgI8AP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.merge(train, train_with_cv_results, on='location_id', how='left')\n",
        "test_df = pd.merge(test, test_with_cv_results, on='location_id', how='left')\n",
        "\n",
        "\n",
        "display(train_df.head(), train_df.shape, test_df.head(), test_df.shape)"
      ],
      "metadata": {
        "id": "KZ9xoR3otFXL",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:09.567111Z",
          "iopub.execute_input": "2025-02-16T02:23:09.567343Z",
          "iopub.status.idle": "2025-02-16T02:23:09.749029Z",
          "shell.execute_reply.started": "2025-02-16T02:23:09.567324Z",
          "shell.execute_reply": "2025-02-16T02:23:09.748186Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = []\n",
        "n_splits = 10\n",
        "gkf = StratifiedGroupKFold(n_splits = n_splits)\n",
        "\n",
        "train_df['fold'] = -1\n",
        "for fold, (_, val_idx) in enumerate(gkf.split(train_df, train_df['label'], groups = train_df['location_id'])):\n",
        "    train_df.loc[val_idx, \"fold\"] = fold\n",
        "\n",
        "train_df['fold'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:09.750447Z",
          "iopub.execute_input": "2025-02-16T02:23:09.750739Z",
          "iopub.status.idle": "2025-02-16T02:23:11.591694Z",
          "shell.execute_reply.started": "2025-02-16T02:23:09.750716Z",
          "shell.execute_reply": "2025-02-16T02:23:11.590910Z"
        },
        "id": "vb0yTv3FimJ4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Feature Engineering"
      ],
      "metadata": {
        "id": "2-LUpXHpEiI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_date_like_features(data):\n",
        "  data['event_t'] = data['event_t'].astype(int)\n",
        "  data['event_t_5_window'] = data['event_t'] // 3\n",
        "  return data"
      ],
      "metadata": {
        "id": "C_W9eqq5lWSj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:11.592446Z",
          "iopub.execute_input": "2025-02-16T02:23:11.592680Z",
          "iopub.status.idle": "2025-02-16T02:23:11.596520Z",
          "shell.execute_reply.started": "2025-02-16T02:23:11.592661Z",
          "shell.execute_reply": "2025-02-16T02:23:11.595701Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.groupby(['location_id'])['label'].agg('count')"
      ],
      "metadata": {
        "id": "FFIRoZ-vzPXZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:11.597593Z",
          "iopub.execute_input": "2025-02-16T02:23:11.597934Z",
          "iopub.status.idle": "2025-02-16T02:23:11.611716Z",
          "shell.execute_reply.started": "2025-02-16T02:23:11.597897Z",
          "shell.execute_reply": "2025-02-16T02:23:11.610838Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot((train['precipitation']))"
      ],
      "metadata": {
        "id": "735dpfV9jWBP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:11.612588Z",
          "iopub.execute_input": "2025-02-16T02:23:11.612901Z",
          "iopub.status.idle": "2025-02-16T02:23:12.294904Z",
          "shell.execute_reply.started": "2025-02-16T02:23:11.612870Z",
          "shell.execute_reply": "2025-02-16T02:23:12.293883Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(np.log1p(test['precipitation']))"
      ],
      "metadata": {
        "id": "udQOJlILjbxg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.295799Z",
          "iopub.execute_input": "2025-02-16T02:23:12.296124Z",
          "iopub.status.idle": "2025-02-16T02:23:12.641572Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.296090Z",
          "shell.execute_reply": "2025-02-16T02:23:12.640496Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['event_t'].describe()"
      ],
      "metadata": {
        "id": "9BTmHt7chCp3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.642559Z",
          "iopub.execute_input": "2025-02-16T02:23:12.642906Z",
          "iopub.status.idle": "2025-02-16T02:23:12.659240Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.642873Z",
          "shell.execute_reply": "2025-02-16T02:23:12.658395Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[train['label']==1]['event_t'].describe()"
      ],
      "metadata": {
        "id": "-ki63210TKP0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.660255Z",
          "iopub.execute_input": "2025-02-16T02:23:12.660580Z",
          "iopub.status.idle": "2025-02-16T02:23:12.670245Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.660539Z",
          "shell.execute_reply": "2025-02-16T02:23:12.669231Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['event_t'].describe()"
      ],
      "metadata": {
        "id": "jJFnfQI6TTkd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.671139Z",
          "iopub.execute_input": "2025-02-16T02:23:12.671373Z",
          "iopub.status.idle": "2025-02-16T02:23:12.693580Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.671353Z",
          "shell.execute_reply": "2025-02-16T02:23:12.692827Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[(train['precipitation']==0) & (train['label']==1)]['event_t'].describe()"
      ],
      "metadata": {
        "id": "b0H2DJzwkkym",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.694464Z",
          "iopub.execute_input": "2025-02-16T02:23:12.694714Z",
          "iopub.status.idle": "2025-02-16T02:23:12.704226Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.694693Z",
          "shell.execute_reply": "2025-02-16T02:23:12.703307Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[(train['precipitation']!=0) & (train['label']==1)]['event_t'].describe()"
      ],
      "metadata": {
        "id": "id93nNj_lD67",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.705047Z",
          "iopub.execute_input": "2025-02-16T02:23:12.705332Z",
          "iopub.status.idle": "2025-02-16T02:23:12.715532Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.705305Z",
          "shell.execute_reply": "2025-02-16T02:23:12.714667Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train['location_id'].nunique()"
      ],
      "metadata": {
        "id": "Pu_dXE2KJEk4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.716702Z",
          "iopub.execute_input": "2025-02-16T02:23:12.716985Z",
          "iopub.status.idle": "2025-02-16T02:23:12.750770Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.716960Z",
          "shell.execute_reply": "2025-02-16T02:23:12.749983Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from types import new_class\n",
        "def apply_expanding_combinations(df, group_cols_list, target_col='Sales', shift_periods=[1], min_periods=1, stats=['mean', 'std']):\n",
        "    # Loop through the group column combinations\n",
        "    for group_cols in group_cols_list:\n",
        "        # Generate base name for the grouping\n",
        "        group_name = '_'.join(group_cols)\n",
        "\n",
        "        for shift_period in shift_periods:\n",
        "            for stat in stats:\n",
        "                expanding_col_name = f'expanding_grouped_{group_name}_{target_col}_shift_{shift_period}_{stat}'\n",
        "\n",
        "                # Apply groupby, shift, and expanding for the given statistic\n",
        "                df[expanding_col_name] = df.groupby(group_cols)[target_col].transform(\n",
        "                    lambda x: x.shift(shift_period).expanding(min_periods=min_periods).agg(stat)\n",
        "                )\n",
        "\n",
        "    return df\n",
        "\n",
        "def smoothen_target(df, group_cols, target_col):\n",
        "  n_std = 10\n",
        "  for i_smooth in [target_col]:\n",
        "      df_id_outlier = df.groupby(group_cols,as_index=False).agg({\n",
        "          f'{i_smooth}': lambda x: x.mean() + n_std*x.std()\n",
        "      }).rename(columns={f'{i_smooth}':f'{i_smooth}_outlier'})\n",
        "\n",
        "      df_id_mean = df.groupby(group_cols,as_index=False).agg({\n",
        "          f'{i_smooth}': 'mean'\n",
        "      }).rename(columns={f'{i_smooth}':f'{i_smooth}_mean'})\n",
        "\n",
        "      df = df.merge(df_id_outlier, on=group_cols[0], how='left')\n",
        "      df = df.merge(df_id_mean, on=group_cols[0], how='left')\n",
        "\n",
        "      df[f'{i_smooth}'] = np.where(\n",
        "          df[f'{i_smooth}'] > df[f'{i_smooth}_outlier'],\n",
        "          df[f'{i_smooth}_mean'],\n",
        "          df[f'{i_smooth}']\n",
        "      )\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def create_rolling_features(data, group_cols, target_col, windows, shift_period, min_period, statistics):\n",
        "    def apply_statistic(x, stat):\n",
        "        rolled = x.shift(shift_period).rolling(window=window, min_periods=min_period)\n",
        "        if stat == 'mean':\n",
        "            return rolled.mean()\n",
        "        elif stat == 'median':\n",
        "            return rolled.median()\n",
        "        elif stat == 'std':\n",
        "            return rolled.std()\n",
        "        elif stat == 'min':\n",
        "            return rolled.min()\n",
        "        elif stat == 'max':\n",
        "            return rolled.max()\n",
        "        elif stat == 'skew':\n",
        "            return rolled.skew()\n",
        "        elif stat == 'sum':\n",
        "            return rolled.sum()\n",
        "        elif stat == 'quantile':\n",
        "            return rolled.quantile(0.95)\n",
        "\n",
        "        elif stat.startswith('quantile_'):\n",
        "            q = float(stat.split('_')[1])\n",
        "            return rolled.quantile(q)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown statistic: {stat}\")\n",
        "\n",
        "    for window in windows:\n",
        "        for stat in statistics:\n",
        "            stat_name = stat if not stat.startswith('quantile_') else f\"{stat.split('_')[1]}th\"\n",
        "            col_name = f'rolling_previous_grouped_{target_col}_{stat_name}_{window}_{shift_period}'\n",
        "\n",
        "            data[col_name] = data.groupby(group_cols)[target_col].transform(\n",
        "                lambda x: apply_statistic(x, stat)\n",
        "            )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def custom_agg(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "def get_date_features(df):\n",
        "  # Simulate year (assuming 365 days per year)\n",
        "  df['year'] = (df['event_t'] // 365) + 1  # Year 1 or 2\n",
        "\n",
        "  # Simulate month (approximate)\n",
        "  df['month'] = ((df['event_t'] % 365) // 30) + 1  # 30-day months approximation\n",
        "\n",
        "  # Simulate week of the year\n",
        "  df['week_of_year'] = (df['event_t'] % 365) // 7 + 1\n",
        "\n",
        "  # Simulate day of the month\n",
        "  df['day_of_month'] = (df['event_t'] % 30) + 1  # Assuming 30-day months\n",
        "\n",
        "  # Simulate day of the week (0 = Monday, 6 = Sunday)\n",
        "  df['day_of_week'] = df['event_t'] % 7\n",
        "\n",
        "  # Simulate quarter\n",
        "  df['quarter'] = ((df['month'] - 1) // 3) + 1\n",
        "  return  df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def feature_engineering(train, test):\n",
        "  data = pd.concat([train, test])\n",
        "  data.sort_values(by = ['location_id', 'event_t'], inplace=True)\n",
        "  data['event_t'] = data['event_t'].astype(int)\n",
        "  # data = smoothen_target(data, ['location_id'], 'precipitation')\n",
        "\n",
        "  data['event_binary'] = data['event_t'].apply(lambda x: 1 if (x >= 296 and x <= 435) else 0)\n",
        "\n",
        "\n",
        "  group_cols =['location_id']\n",
        "  # data = apply_expanding_combinations(\n",
        "  #     data,\n",
        "  #     [group_cols],\n",
        "  #     target_col='precipitation',\n",
        "  #     shift_periods=[1],#1,3, 4, 5, 6, 7, 8, 24\n",
        "  #     min_periods=1,\n",
        "  #     stats=['mean']\n",
        "  # )\n",
        "\n",
        "  statistics = ['mean'] #, 'median', 'std', 'quantile_0.25', 'quantile_0.75'\n",
        "  min_period = 1\n",
        "\n",
        "  shift_period = 0\n",
        "  windows = [3, 4,10,20, 25, 30,55,60, 75, 296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 2\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 3\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 4\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 5\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 6\n",
        "  # windows = [ 3,4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 8\n",
        "  # windows = [ 3,4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "  # # data = get_date_features(data)\n",
        "\n",
        "  for col in ['precipitation']:\n",
        "    # data[f\"grouped_location_{col}_cum\"] = data.groupby('location_id')[col].cumsum().shift(1)\n",
        "\n",
        "    # quantile = 0.95  # Define the quantile you want to calculate\n",
        "    # for stat in ['mean', 'quantile']:\n",
        "    #     if stat != 'quantile':\n",
        "    #         data[f\"location_grouped_{col}_{stat}\"] = data.groupby('location_id')[col].transform(stat)\n",
        "    #         data[f\"diff_{col}_{stat}\"] = data[col] - data[f\"location_grouped_{col}_{stat}\"]\n",
        "\n",
        "\n",
        "    for shift in range(1,365):\n",
        "      data[f'{col}_shift_{shift}'] = data.groupby('location_id')[col].shift(shift)\n",
        "      data[f'{col}_next_shift_{shift}'] = data.groupby('location_id')[col].shift(-shift)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # for window in windows:\n",
        "    #   data[f'{col}_rolling_grouped_custom_{window}'] = (\n",
        "    #       data.groupby('location_id')[col]\n",
        "    #       .rolling(window)\n",
        "    #       .apply(custom_agg)\n",
        "    #       .reset_index(level=0, drop=True)  # Reset the index to align with the original DataFrame\n",
        "    #   )\n",
        "\n",
        "    for span in [7]:\n",
        "        data[f'{col}_ewm_grouped_mean_{span}'] = (\n",
        "            data.groupby('location_id')[col]\n",
        "            .ewm(span=span, adjust=False)\n",
        "            .mean()\n",
        "            .reset_index(level=0, drop=True)  # Reset the index to align it with the original DataFrame\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train = data[data['label'].notna()].reset_index(drop = True)\n",
        "  test = data[data['label'].isna()].reset_index(drop = True)\n",
        "\n",
        "  return train, test\n",
        "\n",
        "new_train, new_test = feature_engineering(train_df, test_df)\n",
        "display(new_train.head(), new_train.shape, new_test.head(), new_test.shape)"
      ],
      "metadata": {
        "id": "H-HFjk91EmcZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:23:12.751672Z",
          "iopub.execute_input": "2025-02-16T02:23:12.751961Z",
          "iopub.status.idle": "2025-02-16T02:24:13.678445Z",
          "shell.execute_reply.started": "2025-02-16T02:23:12.751935Z",
          "shell.execute_reply": "2025-02-16T02:24:13.677807Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(new_test['precipitation'])"
      ],
      "metadata": {
        "id": "t4dZdDyRiEO3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:24:13.679275Z",
          "iopub.execute_input": "2025-02-16T02:24:13.679633Z",
          "iopub.status.idle": "2025-02-16T02:24:14.472253Z",
          "shell.execute_reply.started": "2025-02-16T02:24:13.679581Z",
          "shell.execute_reply": "2025-02-16T02:24:14.471266Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODELLING\n",
        "674 224"
      ],
      "metadata": {
        "id": "7I1N0hGDlPHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_train['label'].value_counts()"
      ],
      "metadata": {
        "id": "6n4Fco4Yt0H0",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:24:14.473141Z",
          "iopub.execute_input": "2025-02-16T02:24:14.473364Z",
          "iopub.status.idle": "2025-02-16T02:24:14.485108Z",
          "shell.execute_reply.started": "2025-02-16T02:24:14.473346Z",
          "shell.execute_reply": "2025-02-16T02:24:14.484191Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_splits):\n",
        "  print(new_train[new_train['fold'] == i]['label'].value_counts())\n",
        "  print(\"-\"* 100)"
      ],
      "metadata": {
        "id": "m3E9y965uGMG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:24:14.488171Z",
          "iopub.execute_input": "2025-02-16T02:24:14.488418Z",
          "iopub.status.idle": "2025-02-16T02:24:15.214238Z",
          "shell.execute_reply.started": "2025-02-16T02:24:14.488396Z",
          "shell.execute_reply": "2025-02-16T02:24:15.213336Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODELLING"
      ],
      "metadata": {
        "id": "PY2g-4yyuhO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test.isnull().sum()"
      ],
      "metadata": {
        "id": "MmKNl_CWz8B_",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:24:15.215348Z",
          "iopub.execute_input": "2025-02-16T02:24:15.215705Z",
          "iopub.status.idle": "2025-02-16T02:24:15.596933Z",
          "shell.execute_reply.started": "2025-02-16T02:24:15.215676Z",
          "shell.execute_reply": "2025-02-16T02:24:15.595948Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "indices_cols = [\n",
        "  'EVI_mean',\n",
        " 'EVI_median',\n",
        " 'EVI_std',\n",
        " 'MNDWI_mean',\n",
        " 'MNDWI_median',\n",
        " 'MNDWI_std',\n",
        " 'MSI_mean',\n",
        " 'MSI_median',\n",
        " 'MSI_std',\n",
        " 'NDVI_mean',\n",
        " 'NDVI_median',\n",
        " 'NDVI_std',\n",
        " 'NDWI_mean',\n",
        " 'NDWI_median',\n",
        " 'NDWI_std',\n",
        " 'Slope_mean',\n",
        " 'Slope_median',\n",
        " 'Slope_std',\n",
        "]\n",
        "\n",
        "selected_columns =['precipitation','flood_probability','event_binary', 'event_t'] + [col for col in new_train if 'diff' in col or 'shift' in col or 'grouped' in col ]\n",
        "\n",
        "print(selected_columns)\n",
        "target_col = 'label'"
      ],
      "metadata": {
        "id": "Nc0a0S2xpNF3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:24:15.597820Z",
          "iopub.execute_input": "2025-02-16T02:24:15.598049Z",
          "iopub.status.idle": "2025-02-16T02:24:15.604876Z",
          "shell.execute_reply.started": "2025-02-16T02:24:15.598030Z",
          "shell.execute_reply": "2025-02-16T02:24:15.603872Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "from scipy.stats import hmean, gmean\n",
        "\n",
        "test_preds_list = []  # Store fold-wise test predictions\n",
        "loglosses = []\n",
        "oof = []\n",
        "\n",
        "new_train = new_train.fillna(0)\n",
        "new_train[selected_columns] = new_train[selected_columns].astype(np.float32)\n",
        "\n",
        "new_test = new_test.fillna(0)\n",
        "new_test[selected_columns] = new_test[selected_columns].astype(np.float32)\n",
        "\n",
        "# K-Fold Training\n",
        "for fold in range(n_splits):\n",
        "    print(f\"============================= TRAINING FOLD: {fold+1} ================================\")\n",
        "\n",
        "    training = new_train[new_train['fold'] != fold]\n",
        "    validation = new_train[new_train['fold'] == fold]\n",
        "\n",
        "\n",
        "    y_train = training[target_col]\n",
        "    y_test = validation[target_col]\n",
        "    X_train = training[selected_columns]\n",
        "    X_test = validation[selected_columns]\n",
        "\n",
        "\n",
        "    # Train the TabNet model\n",
        "    model = TabNetClassifier(n_d = 35, # Width of the decision prediction layer\n",
        "                                n_a = 35, # Width of the attention embedding for each mask\n",
        "                                n_steps = 2, # Number of steps in the architecture\n",
        "                                gamma = 1.3, # coefficient for feature reusage in the masks\n",
        "                                n_independent = 1, # Number of independent Gated Linear Units layers at each step\n",
        "                                n_shared = 2, # Number of shared Gated Linear Units at each step\n",
        "                                momentum = 0.02, # Momentum for batch normalization\n",
        "                                clip_value = None, # extra sparsity loss coefficient\n",
        "                                lambda_sparse = 1e-3,\n",
        "                                optimizer_fn = torch.optim.AdamW, # Adam optimizer\n",
        "                                optimizer_params = dict(lr = 2e-2, weight_decay=0.01), # Optimizer params\n",
        "                                scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
        "                                scheduler_params = {'T_0':5,\n",
        "                                                    'eta_min':1e-3,\n",
        "                                                    'T_mult':1,\n",
        "                                                    'last_epoch':-1},\n",
        "                                mask_type = 'entmax', # masking function to use for selecting features\n",
        "                                device_name='cuda', # Run on gpu\n",
        "                                seed = 2025)\n",
        "\n",
        "    model.fit(X_train.values, y_train.values.ravel(), eval_set = [(X_test.values, y_test.values.ravel())],\n",
        "                  max_epochs = 40,\n",
        "                  patience = 40,\n",
        "                  virtual_batch_size = 256,\n",
        "                  batch_size = 4096,\n",
        "                  eval_metric=['logloss'])\n",
        "\n",
        "    # Store out-of-fold predictions\n",
        "    oof_preds = model.predict_proba(X_test.values)[:, 1]\n",
        "\n",
        "    # Calculate logloss\n",
        "    loss = log_loss(y_test, oof_preds)\n",
        "    print(f\"Logloss: {loss}\")\n",
        "    print(\"*\" * 100)\n",
        "    loglosses.append(loss)\n",
        "\n",
        "    # Inference on the test set\n",
        "    fold_preds = model.predict_proba(new_test[selected_columns].values)[:, 1]\n",
        "    test_preds_list.append(fold_preds)  # Store for alternative means\n",
        "\n",
        "    df = new_train.loc[validation.index.values, ['event_id', 'location_id', 'flood_probability', 'event_t', 'label']].copy()\n",
        "    df['oof_tabnet_pred'] = oof_preds\n",
        "    oof.append(df)\n",
        "\n",
        "\n",
        "# Compute final scores\n",
        "# new_train['oof_correct_prob'] = oof_preds\n",
        "oof = pd.concat(oof, axis=0, ignore_index=True)\n",
        "oof.to_csv(f'tabnet_oof_koleshjr_version{VERSION}.csv', index=False)\n",
        "print(f\"Mean logloss: {np.mean(loglosses)}\")\n",
        "print(f\"OOF logloss: {log_loss(oof['label'], oof['oof_tabnet_pred'])}\")\n",
        "print(f\"OOF roc_auc: {roc_auc_score(oof['label'], oof['oof_tabnet_pred'])}\")\n",
        "\n",
        "# Convert list of fold predictions to NumPy array (shape: n_splits x n_samples)\n",
        "fold_preds_array = np.array(test_preds_list)\n",
        "\n",
        "# Compute different mean types\n",
        "avg_preds_arith = fold_preds_array.mean(axis=0)  # Arithmetic mean\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save different mean-based submissions\n",
        "submission = new_test[['event_id']].copy()\n",
        "submission['tabnet_preds'] = avg_preds_arith\n",
        "submission.to_csv(f'tabnet_version{VERSION}.csv', index=False)"
      ],
      "metadata": {
        "id": "0ODFW6B2P8m1",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T03:02:09.585044Z",
          "iopub.execute_input": "2025-02-16T03:02:09.585331Z",
          "execution_failed": "2025-02-16T03:08:04.655Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the Probabilities based on the Flood Probability"
      ],
      "metadata": {
        "id": "QsRVmeNMimJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "print(f\"logloss before normalizing: {log_loss(oof['label'], oof['oof_tabnet_pred'])}\")\n",
        "\n",
        "locations_to_normalize = oof[oof['flood_probability'] >= 0.5]['location_id'].unique()\n",
        "oof['oof_sum_prob'] = oof.groupby('location_id')['oof_tabnet_pred'].transform('sum')\n",
        "\n",
        "# Avoid division by zero\n",
        "epsilon = 1e-8\n",
        "oof['oof_tabnet_norm'] = oof['oof_tabnet_pred']  # Copy original values\n",
        "\n",
        "oof.loc[oof['location_id'].isin(locations_to_normalize), 'oof_tabnet_norm'] = (\n",
        "    oof.loc[oof['location_id'].isin(locations_to_normalize), 'oof_tabnet_pred'] /\n",
        "    (oof.loc[oof['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n",
        ")\n",
        "\n",
        "print(f\"logloss after normalizing: {log_loss(oof['label'], oof['oof_tabnet_norm'])}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T02:38:19.096332Z",
          "iopub.execute_input": "2025-02-16T02:38:19.096677Z",
          "iopub.status.idle": "2025-02-16T02:38:19.148756Z",
          "shell.execute_reply.started": "2025-02-16T02:38:19.096640Z",
          "shell.execute_reply": "2025-02-16T02:38:19.147996Z"
        },
        "id": "71nc8igYimJ8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}