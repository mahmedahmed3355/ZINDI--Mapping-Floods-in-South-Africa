{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9984259,
          "sourceType": "datasetVersion",
          "datasetId": 6144078
        },
        {
          "sourceId": 10716224,
          "sourceType": "datasetVersion",
          "datasetId": 6642380
        },
        {
          "sourceId": 10776345,
          "sourceType": "datasetVersion",
          "datasetId": 6682996
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4415.742129,
      "end_time": "2025-02-03T18:23:51.332918",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-02-03T17:10:15.590789",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Directory Settings"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.010628,
          "end_time": "2025-02-03T17:10:17.906454",
          "exception": false,
          "start_time": "2025-02-03T17:10:17.895826",
          "status": "completed"
        },
        "tags": [],
        "id": "8yD_J9JZjkVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# directory settings\n",
        "# ====================================================\n",
        "\n",
        "import os\n",
        "\n",
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2025-02-17T17:18:20.896880Z",
          "iopub.execute_input": "2025-02-17T17:18:20.897243Z",
          "iopub.status.idle": "2025-02-17T17:18:20.903570Z",
          "shell.execute_reply.started": "2025-02-17T17:18:20.897208Z",
          "shell.execute_reply": "2025-02-17T17:18:20.902288Z"
        },
        "papermill": {
          "duration": 0.012607,
          "end_time": "2025-02-03T17:10:17.92511",
          "exception": false,
          "start_time": "2025-02-03T17:10:17.912503",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "hEm9sHorjkVt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005325,
          "end_time": "2025-02-03T17:10:17.93543",
          "exception": false,
          "start_time": "2025-02-03T17:10:17.930105",
          "status": "completed"
        },
        "tags": [],
        "id": "Ok0-6ioVjkVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "from glob import glob\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "from scipy.stats import entropy\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "from contextlib import contextmanager\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, PowerTransformer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, AdamW\n",
        "import torchvision.models as models\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchvision.transforms import v2\n",
        "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import albumentations as A\n",
        "from albumentations import (Compose, Normalize, Resize, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate, Transpose)\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations import ImageOnlyTransform\n",
        "import timm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from matplotlib import pyplot as plt\n",
        "import joblib\n",
        "VERSION=17\n",
        "base_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"\n",
        "additional_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/image_classifier_results/\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-17T17:18:20.904714Z",
          "iopub.execute_input": "2025-02-17T17:18:20.905158Z",
          "iopub.status.idle": "2025-02-17T17:18:43.524178Z",
          "shell.execute_reply.started": "2025-02-17T17:18:20.905127Z",
          "shell.execute_reply": "2025-02-17T17:18:43.522697Z"
        },
        "papermill": {
          "duration": 12.090314,
          "end_time": "2025-02-03T17:10:30.030651",
          "exception": false,
          "start_time": "2025-02-03T17:10:17.940337",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "X6YiJaZGjkVu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005059,
          "end_time": "2025-02-03T17:10:30.041129",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.03607",
          "status": "completed"
        },
        "tags": [],
        "id": "WlzRTbmyjkVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "\n",
        "class CFG:\n",
        "    wandb = False\n",
        "    debug = False\n",
        "    train=True\n",
        "    apex=True\n",
        "    t4_gpu=False\n",
        "    scheduler='OneCycleLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n",
        "    # CosineAnnealingLR params\n",
        "    cosanneal_params={\n",
        "        'T_max':6,\n",
        "        'eta_min':1e-5,\n",
        "        'last_epoch':-1\n",
        "    }\n",
        "    #ReduceLROnPlateau params\n",
        "    reduce_params={\n",
        "        'mode':'min',\n",
        "        'factor':0.2,\n",
        "        'patience':4,\n",
        "        'eps':1e-6,\n",
        "        'verbose':True\n",
        "    }\n",
        "    # CosineAnnealingWarmRestarts params\n",
        "    cosanneal_res_params={\n",
        "        'T_0':20,\n",
        "        'eta_min':1e-6,\n",
        "        'T_mult':1,\n",
        "        'last_epoch':-1\n",
        "    }\n",
        "    print_freq=15\n",
        "    num_workers = 1\n",
        "    cnn_model_name = 'resnet1d'\n",
        "    model_name = 'resnet1d'\n",
        "    optimizer='Adan'\n",
        "    epochs = 25\n",
        "    factor = 0.9\n",
        "    patience = 2\n",
        "    eps = 1e-6\n",
        "    lr = 1e-3\n",
        "    min_lr = 1e-6\n",
        "    batch_size = 32\n",
        "    weight_decay = 1e-2\n",
        "    batch_scheduler=True\n",
        "    gradient_accumulation_steps = 1\n",
        "    max_grad_norm = 1e6\n",
        "    seed = 2025\n",
        "    target_cols = \"label\"\n",
        "    target_size = 1\n",
        "    in_channels = 1\n",
        "    n_fold = 10\n",
        "    # trn_fold = [2]\n",
        "    trn_fold = [4, 5, 6]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-17T17:18:52.106908Z",
          "iopub.execute_input": "2025-02-17T17:18:52.107324Z",
          "iopub.status.idle": "2025-02-17T17:18:52.114605Z",
          "shell.execute_reply.started": "2025-02-17T17:18:52.107295Z",
          "shell.execute_reply": "2025-02-17T17:18:52.113387Z"
        },
        "papermill": {
          "duration": 0.011931,
          "end_time": "2025-02-03T17:10:30.057967",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.046036",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "o3uawqNzjkVu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.004914,
          "end_time": "2025-02-03T17:10:30.067882",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.062968",
          "status": "completed"
        },
        "tags": [],
        "id": "i_AVwJz_jkVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "def get_score(preds, targets):\n",
        "\n",
        "    return log_loss(targets, preds)\n",
        "\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive loss function.\n",
        "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        output1_norm = F.normalize(output1)\n",
        "        output2_norm = F.normalize(output2)\n",
        "        euclidean_distance = F.cosine_similarity(output1_norm, output2_norm)\n",
        "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
        "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "        return loss_contrastive\n",
        "\n",
        "def get_location(value):\n",
        "  return value.split(\"_\")[0] + '_' + value.split(\"_\")[1]\n",
        "\n",
        "\n",
        "# def seed_torch(seed=42):\n",
        "#     random.seed(seed)\n",
        "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "#     np.random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed(seed)\n",
        "#     torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "#     torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def seed_torch(seed=CFG.seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "seed_torch(seed=CFG.seed)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-17T17:18:52.701847Z",
          "iopub.execute_input": "2025-02-17T17:18:52.702255Z",
          "iopub.status.idle": "2025-02-17T17:18:52.720519Z",
          "shell.execute_reply.started": "2025-02-17T17:18:52.702225Z",
          "shell.execute_reply": "2025-02-17T17:18:52.719464Z"
        },
        "papermill": {
          "duration": 0.022753,
          "end_time": "2025-02-03T17:10:30.095541",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.072788",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "2kOBqFDajkVv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.004764,
          "end_time": "2025-02-03T17:10:30.105527",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.100763",
          "status": "completed"
        },
        "tags": [],
        "id": "UiO4tzIzjkVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load precipitation data"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.004752,
          "end_time": "2025-02-03T17:10:30.115279",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.110527",
          "status": "completed"
        },
        "tags": [],
        "id": "dlA87VJ0jkVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(base_path + \"Train.csv\")\n",
        "data_test = pd.read_csv(base_path + \"Test.csv\")\n",
        "data_with_cv = pd.read_csv(additional_path + \"train_with_cv_results.csv\")[['location_id', 'flood_probability']]\n",
        "data_test_with_cv = pd.read_csv(additional_path + \"test_with_cv_results.csv\")[['location_id', 'flood_probability',]]\n",
        "\n",
        "for df in [data, data_test]:\n",
        "\n",
        "  df['location_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
        "  df['event_idx'] = df.groupby('location_id', sort=False).ngroup()\n",
        "\n",
        "  df['event_t'] = df.groupby('location_id').cumcount()\n",
        "\n",
        "\n",
        "data = pd.merge(data, data_with_cv, on='location_id', how='left')\n",
        "data_test = pd.merge(data_test, data_test_with_cv, on='location_id', how='left')\n",
        "\n",
        "\n",
        "print(len(set(data['location_id'])), len(set(data_test['location_id'])))\n",
        "print(len(set(data['location_id']).intersection(set(data_test['location_id']))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-17T17:18:57.218856Z",
          "iopub.execute_input": "2025-02-17T17:18:57.219267Z",
          "iopub.status.idle": "2025-02-17T17:18:59.108514Z",
          "shell.execute_reply.started": "2025-02-17T17:18:57.219238Z",
          "shell.execute_reply": "2025-02-17T17:18:59.107323Z"
        },
        "papermill": {
          "duration": 1.04691,
          "end_time": "2025-02-03T17:10:31.167109",
          "exception": false,
          "start_time": "2025-02-03T17:10:30.120199",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "TG5rSx6ijkVw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-17T17:18:59.109778Z",
          "iopub.execute_input": "2025-02-17T17:18:59.110110Z",
          "iopub.status.idle": "2025-02-17T17:18:59.132628Z",
          "shell.execute_reply.started": "2025-02-17T17:18:59.110084Z",
          "shell.execute_reply": "2025-02-17T17:18:59.131392Z"
        },
        "id": "xGw2iRjPjkVw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gkf = StratifiedGroupKFold(n_splits=CFG.n_fold)\n",
        "\n",
        "data['fold'] = -1\n",
        "\n",
        "for fold_id, (_, val_idx) in enumerate(\n",
        "    gkf.split(data, y=data['label'], groups=data['location_id'])\n",
        "):\n",
        "    print(f\"Fold {fold_id}\")\n",
        "    print(f\"group={data['location_id'][val_idx].unique()}\")\n",
        "    data.loc[val_idx, 'fold'] = fold_id\n",
        "\n",
        "del gkf\n",
        "_ = gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-17T17:19:01.266016Z",
          "iopub.execute_input": "2025-02-17T17:19:01.266499Z",
          "iopub.status.idle": "2025-02-17T17:19:03.906911Z",
          "shell.execute_reply.started": "2025-02-17T17:19:01.266458Z",
          "shell.execute_reply": "2025-02-17T17:19:03.906011Z"
        },
        "papermill": {
          "duration": 0.680728,
          "end_time": "2025-02-03T17:10:31.853572",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.172844",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "GcihQe1LjkVw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def norm_feature_engineering(df):\n",
        "\n",
        "    # df = apply_boxcox_transformation(df, \"precipitation\")\n",
        "\n",
        "    df['precipitation'] = np.log(df[\"precipitation\"] + 0.01).astype(\"float32\")\n",
        "\n",
        "\n",
        "    for w in range(1, 182):\n",
        "        df['precipitation_shift_pos_' + str(w)] = df['precipitation'].shift(w).fillna(0)\n",
        "        df['precipitation_shift_neg_' + str(w)] = df['precipitation'].shift(-w).fillna(0)\n",
        "\n",
        "    for w in [3, 7, 14, 30, 60, 120]:\n",
        "        df[f'precipitation_rolling_mean_{w}'] = df['precipitation'].rolling(w, min_periods=1).mean()\n",
        "        df[f'precipitation_rolling_std_{w}'] = df['precipitation'].rolling(w, min_periods=1).std()\n",
        "        df[f'precipitation_rolling_median_{w}'] = df['precipitation'].rolling(w, min_periods=1).median()\n",
        "\n",
        "\n",
        "    for w in [3, 7, 14, 30, 60, 90, 120, 180]:\n",
        "        df[f'precipitation_max_{w}'] = df['precipitation'].rolling(w, min_periods=1).max()\n",
        "        df[f'precipitation_min_{w}'] = df['precipitation'].rolling(w, min_periods=1).min()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    df = df[[col for col in df.columns if 'precipitation' in col] + ['flood_probability']].fillna(0)\n",
        "\n",
        "    return df.to_numpy().transpose(1, 0)\n",
        "\n",
        "def apply_boxcox_transformation(df, col):\n",
        "    \"\"\"\n",
        "    Applies the Box-Cox transformation to a specified column while handling zero values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame.\n",
        "        col (str): Column name to transform.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with the transformed column.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    pt = PowerTransformer(method='box-cox')\n",
        "\n",
        "    # Mask for non-zero values\n",
        "    non_zero_mask = df[col] != 0.0\n",
        "\n",
        "    # Initialize transformed data array\n",
        "    transformed_data = np.zeros_like(df[col], dtype=float)\n",
        "\n",
        "    # Apply transformation to non-zero values\n",
        "    non_zero_transformed = pt.fit_transform(df.loc[non_zero_mask, [col]]).flatten()\n",
        "\n",
        "    # Assign transformed values\n",
        "    min_value = non_zero_transformed.min()\n",
        "    transformed_data[non_zero_mask] = non_zero_transformed\n",
        "    transformed_data[~non_zero_mask] = min_value - 0.001  # Slightly below min\n",
        "\n",
        "    # Assign back to DataFrame\n",
        "    df[col] = transformed_data\n",
        "    return df\n",
        "\n",
        "def time_warp(features, sigma=0.2, knot=4):\n",
        "    \"\"\"\n",
        "    Apply time warping to each feature independently.\n",
        "    \"\"\"\n",
        "    warped_features = np.zeros_like(features)\n",
        "    orig_steps = np.arange(features.shape[1])\n",
        "\n",
        "    for i in range(features.shape[0]):  # Iterate over features (17)\n",
        "        random_warp = np.random.normal(loc=1.0, scale=sigma, size=(knot,))\n",
        "        interp = interp1d(np.linspace(0, features.shape[1], num=knot), random_warp, kind='linear', fill_value='extrapolate')\n",
        "        warped_series = interp(orig_steps)\n",
        "        warped_features[i] = np.interp(orig_steps * warped_series, orig_steps, features[i])\n",
        "\n",
        "    return warped_features\n",
        "\n",
        "def time_shift(features, max_shift=10):\n",
        "    shift = np.random.randint(-max_shift, max_shift)\n",
        "    return np.roll(features, shift, axis=1)  # Shift along the time dimension\n",
        "\n",
        "\n",
        "def add_gaussian_noise(features, std=0.1):\n",
        "    noise = np.random.normal(0, std, size=features.shape)\n",
        "    return features + noise\n",
        "\n",
        "def feature_dropout(features, drop_prob=0.2):\n",
        "    mask = np.random.binomial(1, 1 - drop_prob, (features.shape[0], 1))  # Same mask for all time steps\n",
        "    return features * mask\n",
        "\n",
        "def scale_features(features, scale_range=(0.8, 1.2)):\n",
        "    scales = np.random.uniform(scale_range[0], scale_range[1], size=(features.shape[0], 1))\n",
        "    return features * scales\n",
        "\n",
        "def freq_perturbation(features, alpha=0.1):\n",
        "    fft_coeffs = np.fft.fft(features, axis=1)  # Apply FFT along time dimension\n",
        "    perturb = np.random.normal(1, alpha, size=fft_coeffs.shape)\n",
        "    return np.real(np.fft.ifft(fft_coeffs * perturb, axis=1))  # Apply inverse FFT\n",
        "\n",
        "def random_flip(precipitation, label):\n",
        "    precipitation = np.flip(precipitation).copy()  # Ensure positive strides\n",
        "    label = np.flip(label).copy()  # Ensure positive strides\n",
        "    return precipitation, label\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(\n",
        "        self, df: pd.DataFrame, augment: bool = False,\n",
        "        mode: bool = True\n",
        "    ):\n",
        "        self.df = df\n",
        "        self.augment = augment\n",
        "        self.mode = mode\n",
        "        self.location_ids = df['location_id'].unique()\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df['location_id'].unique())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        precipitation, label = self.__data_generation(index)\n",
        "\n",
        "        if self.augment:\n",
        "            if np.random.rand() < 0.3:\n",
        "                precipitation = time_shift(precipitation)\n",
        "\n",
        "            if np.random.rand() < 0.3:\n",
        "                precipitation = add_gaussian_noise(precipitation)\n",
        "\n",
        "            if np.random.rand() < 0.3:\n",
        "                precipitation = feature_dropout(precipitation)\n",
        "\n",
        "            if np.random.rand() < 0.3:\n",
        "                precipitation, label = random_flip(precipitation, label)\n",
        "\n",
        "        return {'precipitation': torch.tensor(precipitation, dtype=torch.float32), 'label': torch.tensor(label, dtype=torch.float32)}\n",
        "\n",
        "    def __data_generation(self, index):\n",
        "\n",
        "        event_id = self.location_ids[index]\n",
        "        # precipitation = self.df[self.df['event_id'] == event_id].pivot(index='event_id', columns='event_t', values='precipitation').to_numpy()\n",
        "        precipitation = self.df[self.df['location_id'] == event_id].sort_values('event_t')\n",
        "        precipitation = norm_feature_engineering(precipitation)\n",
        "\n",
        "        label = np.zeros(730, dtype='float32')\n",
        "        if self.mode != 'test':\n",
        "            label = self.df[self.df['location_id'] == event_id].pivot(index='location_id', columns='event_t', values='label').to_numpy()\n",
        "            label = np.squeeze(label, axis=0)\n",
        "\n",
        "        return precipitation, label"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:01.703686Z",
          "iopub.execute_input": "2025-02-16T16:42:01.703925Z",
          "iopub.status.idle": "2025-02-16T16:42:01.720714Z",
          "shell.execute_reply.started": "2025-02-16T16:42:01.703905Z",
          "shell.execute_reply": "2025-02-16T16:42:01.719926Z"
        },
        "papermill": {
          "duration": 0.019317,
          "end_time": "2025-02-03T17:10:31.878692",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.859375",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "at7zRpLCjkVw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# W&B Settings"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005031,
          "end_time": "2025-02-03T17:10:31.889319",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.884288",
          "status": "completed"
        },
        "tags": [],
        "id": "ogpyA-07jkVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# wandb\n",
        "# ====================================================\n",
        "if CFG.wandb:\n",
        "\n",
        "    import wandb\n",
        "\n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        user_secrets = UserSecretsClient()\n",
        "        secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n",
        "        wandb.login(key=secret_value_0)\n",
        "        anony = None\n",
        "    except:\n",
        "        anony = \"must\"\n",
        "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
        "\n",
        "\n",
        "    def class2dict(f):\n",
        "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:01.72165Z",
          "iopub.execute_input": "2025-02-16T16:42:01.721984Z",
          "iopub.status.idle": "2025-02-16T16:42:01.739848Z",
          "shell.execute_reply.started": "2025-02-16T16:42:01.72195Z",
          "shell.execute_reply": "2025-02-16T16:42:01.738737Z"
        },
        "papermill": {
          "duration": 0.011174,
          "end_time": "2025-02-03T17:10:31.905662",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.894488",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "1fKOLA3qjkVx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005014,
          "end_time": "2025-02-03T17:10:31.916003",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.910989",
          "status": "completed"
        },
        "tags": [],
        "id": "RqqX1qB7jkVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionalEmbedding(torch.nn.Module):\n",
        "    def __init__(self, seq_len, dim_model):\n",
        "        \"\"\"\n",
        "        Initialize the sinusoidal positional embedding.\n",
        "\n",
        "        Args:\n",
        "        seq_len (int): The length of the sequence (e.g., 730 days).\n",
        "        dim_model (int): The model's embedding size (e.g., 512).\n",
        "        \"\"\"\n",
        "        super(SinusoidalPositionalEmbedding, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.dim_model = dim_model\n",
        "        self.positional_embedding = self._create_positional_embedding()\n",
        "\n",
        "    def _create_positional_embedding(self):\n",
        "        \"\"\"\n",
        "        Create the sinusoidal positional embedding tensor.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: Positional embeddings of shape (seq_len, dim_model).\n",
        "        \"\"\"\n",
        "        position = torch.arange(self.seq_len, dtype=torch.float).unsqueeze(1)  # Shape: (seq_len, 1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, self.dim_model, 2, dtype=torch.float) *\n",
        "            (-math.log(10000.0) / self.dim_model)\n",
        "        )  # Frequencies: Shape (dim_model / 2)\n",
        "\n",
        "        # Compute sin and cos\n",
        "        pos_embedding = torch.zeros(self.seq_len, self.dim_model)\n",
        "        pos_embedding[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n",
        "        pos_embedding[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n",
        "\n",
        "        return pos_embedding\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add positional embedding to input tensor.\n",
        "\n",
        "        Args:\n",
        "        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim_model).\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: Input tensor with positional embeddings added.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, dim_model = x.size()\n",
        "        if seq_len != self.seq_len or dim_model != self.dim_model:\n",
        "            raise ValueError(\n",
        "                f\"Input shape mismatch: Expected (batch_size, {self.seq_len}, {self.dim_model}), got {x.size()}\"\n",
        "            )\n",
        "        return x + self.positional_embedding.to(x.device).unsqueeze(0)  # Broadcast positional embeddings\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for channel attention.\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        b, c, _ = x.size()\n",
        "        y = self.global_avg_pool(x).view(b, c)\n",
        "\n",
        "        # Excitation: Fully Connected Layers\n",
        "        y = self.fc1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.fc2(y)\n",
        "        y = self.sigmoid(y).view(b, c, 1)\n",
        "\n",
        "        # Scale\n",
        "        return x * y\n",
        "\n",
        "class ResNetBlock1d(nn.Module):\n",
        "    \"\"\"ResNet block for 1D convolutions with Squeeze-and-Excitation.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, filters, se_reduction=16):\n",
        "        super(ResNetBlock1d, self).__init__()\n",
        "        self.filters = filters\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=filters,\n",
        "            kernel_size=3,\n",
        "            stride=1,  # Preserve feature dimension\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(filters)\n",
        "        self.conv2 = nn.Conv1d(\n",
        "            in_channels=filters,\n",
        "            out_channels=filters,\n",
        "            kernel_size=3,\n",
        "            stride=1,  # Preserve feature dimension\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(filters)\n",
        "\n",
        "        # SE Block\n",
        "        self.se_block = SEBlock(filters, reduction=se_reduction)\n",
        "\n",
        "        # Activation\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        # Projection for residual if in_channels != filters\n",
        "        if in_channels != filters:\n",
        "            self.projection = nn.Sequential(\n",
        "                nn.Conv1d(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=filters,\n",
        "                    kernel_size=1,\n",
        "                    stride=1,  # Preserve feature dimension\n",
        "                    bias=False\n",
        "                ),\n",
        "                nn.BatchNorm1d(filters)\n",
        "            )\n",
        "        else:\n",
        "            self.projection = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        # Main path\n",
        "        y = self.conv1(x)\n",
        "        y = self.bn1(y)\n",
        "        y = self.act(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        y = self.se_block(y)\n",
        "\n",
        "        # Projection if necessary\n",
        "        if self.projection is not None:\n",
        "            residual = self.projection(x)\n",
        "\n",
        "        return self.act(residual + y)\n",
        "\n",
        "\n",
        "class ResNet1d(nn.Module):\n",
        "    \"\"\"ResNet for 1D convolutions, returns embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, stage_sizes, num_filters=64, embed=True):\n",
        "        super(ResNet1d, self).__init__()\n",
        "        self.num_filters = num_filters\n",
        "        self.stage_sizes = stage_sizes\n",
        "        self.embed = embed\n",
        "\n",
        "        # Initial layers\n",
        "        self.conv_init = nn.Conv1d(\n",
        "            in_channels=398,\n",
        "            out_channels=num_filters,\n",
        "            kernel_size=7,\n",
        "            stride=1,  # Preserve feature dimension\n",
        "            padding=3,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn_init = nn.BatchNorm1d(num_filters)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        self.project_embedding = nn.Linear(398, 576)\n",
        "        self.pos_emb = SinusoidalPositionalEmbedding(730, 576)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=576, nhead=576//32, dim_feedforward=2*576,\n",
        "                dropout=0.2, activation=nn.GELU(), batch_first=True, norm_first=True), 2)\n",
        "\n",
        "        # self.attn = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)\n",
        "        self.rnn1 = nn.GRU(input_size=398, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.rnn2 = nn.GRU(input_size=398, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.logit = nn.Conv1d(in_channels=576, out_channels=1, kernel_size=7, padding=3, stride=1)\n",
        "        # Build ResNet stages\n",
        "        self.stages = self._make_stages()\n",
        "\n",
        "    def _make_stages(self):\n",
        "        stages = nn.ModuleList()\n",
        "        in_channels = self.num_filters\n",
        "\n",
        "        for i, block_size in enumerate(self.stage_sizes):\n",
        "            blocks = nn.ModuleList()\n",
        "            for j in range(block_size):\n",
        "                filters = self.num_filters * (2 ** i)\n",
        "                blocks.append(ResNetBlock1d(in_channels=in_channels, filters=filters))\n",
        "                in_channels = filters  # Update in_channels for the next block\n",
        "            stages.append(blocks)\n",
        "\n",
        "        return stages\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        project_precipitation = self.project_embedding(x.permute(0, 2, 1))\n",
        "        # x = x.unsqueeze(1)  # Add channel dimension\n",
        "        out = self.conv_init(x)\n",
        "        out = self.bn_init(out)\n",
        "        out = self.act(out)\n",
        "\n",
        "        for i, blocks in enumerate(self.stages):\n",
        "            for j, block in enumerate(blocks):\n",
        "                out = block(out)\n",
        "        rnn_out1, hidden = self.rnn1(x.permute(0, 2, 1))\n",
        "        rnn_out2, hidden2 = self.rnn2(x.permute(0, 2, 1), hidden)\n",
        "        # rnn_out2, _ = self.attn(rnn_out2, rnn_out2, rnn_out2)\n",
        "        new_out1 = torch.cat([out.permute(0, 2, 1), rnn_out2], dim=2)\n",
        "\n",
        "        pos_embedding = self.pos_emb(project_precipitation)\n",
        "        transformer_input = pos_embedding  + new_out1\n",
        "        transformer_fusion = self.transformer(transformer_input)\n",
        "        return new_out1, transformer_fusion\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, x = self.extract_features(x)\n",
        "        x = self.logit(x.permute(0, 2, 1))\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:01.740704Z",
          "iopub.execute_input": "2025-02-16T16:42:01.7409Z",
          "iopub.status.idle": "2025-02-16T16:42:01.760661Z",
          "shell.execute_reply.started": "2025-02-16T16:42:01.740883Z",
          "shell.execute_reply": "2025-02-16T16:42:01.759739Z"
        },
        "papermill": {
          "duration": 0.025127,
          "end_time": "2025-02-03T17:10:31.94649",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.921363",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "ZpvEMq_mjkVx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if CFG.debug:\n",
        "\n",
        "    train_data = data[data['fold'] == 0]\n",
        "\n",
        "    dataset = CustomDataset(train_data, augment=True, mode='valid')\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            batch_size=CFG.batch_size,\n",
        "                            shuffle=True,\n",
        "                            num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "\n",
        "    batch = next(iter(dataloader))\n",
        "\n",
        "\n",
        "\n",
        "    model = ResNet1d(stage_sizes=[2, 3, 3], num_filters=16).to(device)\n",
        "\n",
        "    pred_precipitation = model(batch['precipitation'].to(device))\n",
        "\n",
        "\n",
        "    criteria = nn.BCEWithLogitsLoss()\n",
        "    loss = criteria(pred_precipitation.squeeze(1), batch['label'].to(device))\n",
        "    # contrastive_target = torch.zeros(image_emb.size(0)).to(device)  # Assuming all pairs are not similar\n",
        "\n",
        "    # assert torch.all(torch.isfinite(image_emb)), \"Non-finite values in output1\"\n",
        "    # assert torch.all(torch.isfinite(precipitation_emb)), \"Non-finite values in output2\"\n",
        "    # assert torch.all((contrastive_target == 0) | (contrastive_target == 1)), \"Labels must be 0 or 1\"\n",
        "    # contrastive_loss = ContrastiveLoss()(image_emb, precipitation_emb, contrastive_target)\n",
        "\n",
        "    metric = log_loss(batch['label'].flatten().detach().cpu().numpy(), pred_precipitation.sigmoid().flatten().detach().cpu().numpy())\n",
        "\n",
        "    # print(\"early_fusion: \", early_fusion.shape)\n",
        "    # print(\"pred_image: \", pred_image.shape)\n",
        "    print(\"pred_precipitation: \", pred_precipitation.shape)\n",
        "    # print(\"image_emb: \", image_emb.shape)\n",
        "    # print(\"precipitation_emb: \", precipitation_emb.shape)\n",
        "    print(\"loss: \", loss.item())\n",
        "    # print(\"contrastive loss: \", contrastive_loss.item())\n",
        "    print(\"metric: \", metric)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:01.762303Z",
          "iopub.execute_input": "2025-02-16T16:42:01.762626Z",
          "iopub.status.idle": "2025-02-16T16:42:10.090255Z",
          "shell.execute_reply.started": "2025-02-16T16:42:01.762596Z",
          "shell.execute_reply": "2025-02-16T16:42:10.089412Z"
        },
        "papermill": {
          "duration": 0.011767,
          "end_time": "2025-02-03T17:10:31.963494",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.951727",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "Ul9uYTebjkVy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adan Optimizer"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.004998,
          "end_time": "2025-02-03T17:10:31.973735",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.968737",
          "status": "completed"
        },
        "tags": [],
        "id": "Q40zX3VtjkVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "\n",
        "class Adan(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements a pytorch variant of Adan\n",
        "    Adan was proposed in\n",
        "    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n",
        "    https://arxiv.org/abs/2208.06677\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n",
        "        lr (float, optional): learning rate. (default: 1e-3)\n",
        "        betas (Tuple[float, float, flot], optional): coefficients used for computing\n",
        "            running averages of gradient and its norm. (default: (0.98, 0.92, 0.99))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability. (default: 1e-8)\n",
        "        weight_decay (float, optional): decoupled weight decay (L2 penalty) (default: 0)\n",
        "        max_grad_norm (float, optional): value used to clip\n",
        "            global grad norm (default: 0.0 no clip)\n",
        "        no_prox (bool): how to perform the decoupled weight decay (default: False)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.98, 0.92, 0.99), eps=1e-8,\n",
        "                 weight_decay=0.2, max_grad_norm=0.0, no_prox=False):\n",
        "        if not 0.0 <= max_grad_norm:\n",
        "            raise ValueError(\"Invalid Max grad norm: {}\".format(max_grad_norm))\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        if not 0.0 <= betas[2] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay,\n",
        "                        max_grad_norm=max_grad_norm, no_prox=no_prox)\n",
        "        super(Adan, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adan, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('no_prox', False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restart_opt(self):\n",
        "        for group in self.param_groups:\n",
        "            group['step'] = 0\n",
        "            for p in group['params']:\n",
        "                if p.requires_grad:\n",
        "                    state = self.state[p]\n",
        "                    # State initialization\n",
        "\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "                    # Exponential moving average of gradient difference\n",
        "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "            Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        if self.defaults['max_grad_norm'] > 0:\n",
        "            device = self.param_groups[0]['params'][0].device\n",
        "            global_grad_norm = torch.zeros(1, device=device)\n",
        "\n",
        "            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\n",
        "            for group in self.param_groups:\n",
        "\n",
        "                for p in group['params']:\n",
        "                    if p.grad is not None:\n",
        "                        grad = p.grad\n",
        "                        global_grad_norm.add_(grad.pow(2).sum())\n",
        "\n",
        "            global_grad_norm = torch.sqrt(global_grad_norm)\n",
        "\n",
        "            clip_global_grad_norm = torch.clamp(max_grad_norm / (global_grad_norm + group['eps']), max=1.0)\n",
        "        else:\n",
        "            clip_global_grad_norm = 1.0\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2, beta3 = group['betas']\n",
        "            # assume same step across group now to simplify things\n",
        "            # per parameter step can be easily support by making it tensor, or pass list into kernel\n",
        "            if 'step' in group:\n",
        "                group['step'] += 1\n",
        "            else:\n",
        "                group['step'] = 1\n",
        "\n",
        "            bias_correction1 = 1.0 - beta1 ** group['step']\n",
        "\n",
        "            bias_correction2 = 1.0 - beta2 ** group['step']\n",
        "\n",
        "            bias_correction3 = 1.0 - beta3 ** group['step']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "                    state['exp_avg_diff'] = torch.zeros_like(p)\n",
        "\n",
        "                grad = p.grad.mul_(clip_global_grad_norm)\n",
        "                if 'pre_grad' not in state or group['step'] == 1:\n",
        "                    state['pre_grad'] = grad\n",
        "\n",
        "                copy_grad = grad.clone()\n",
        "\n",
        "                exp_avg, exp_avg_sq, exp_avg_diff = state['exp_avg'], state['exp_avg_sq'], state['exp_avg_diff']\n",
        "                diff = grad - state['pre_grad']\n",
        "\n",
        "                update = grad + beta2 * diff\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n",
        "                exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  # diff_t\n",
        "                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  # n_t\n",
        "\n",
        "                denom = ((exp_avg_sq).sqrt() / math.sqrt(bias_correction3)).add_(group['eps'])\n",
        "                update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)\n",
        "\n",
        "                if group['no_prox']:\n",
        "                    p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "                    p.add_(update, alpha=-group['lr'])\n",
        "                else:\n",
        "                    p.add_(update, alpha=-group['lr'])\n",
        "                    p.data.div_(1 + group['lr'] * group['weight_decay'])\n",
        "\n",
        "                state['pre_grad'] = copy_grad"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:10.091201Z",
          "iopub.execute_input": "2025-02-16T16:42:10.091526Z",
          "iopub.status.idle": "2025-02-16T16:42:10.105799Z",
          "shell.execute_reply.started": "2025-02-16T16:42:10.091488Z",
          "shell.execute_reply": "2025-02-16T16:42:10.104921Z"
        },
        "papermill": {
          "duration": 0.020942,
          "end_time": "2025-02-03T17:10:31.999924",
          "exception": false,
          "start_time": "2025-02-03T17:10:31.978982",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "_kg_hide-input": true,
        "id": "5tv5BdA6jkVy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.005077,
          "end_time": "2025-02-03T17:10:32.010313",
          "exception": false,
          "start_time": "2025-02-03T17:10:32.005236",
          "status": "completed"
        },
        "tags": [],
        "id": "BAd3VmFLjkVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# Helper functions\n",
        "# ====================================================\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
        "    model.train()\n",
        "    scaler = torch.amp.GradScaler('cuda', enabled=CFG.apex)\n",
        "    losses = AverageMeter()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        precipitation = batch['precipitation'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        with torch.amp.autocast('cuda', enabled=CFG.apex):\n",
        "\n",
        "            pred_precipitation = model(precipitation)\n",
        "\n",
        "            #loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
        "            loss = criterion(pred_precipitation.squeeze(1), labels)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        scaler.scale(loss).backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            if CFG.batch_scheduler:\n",
        "                scheduler.step()\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  'LR: {lr:.8f}  '\n",
        "                  .format(epoch+1, step, len(train_loader),\n",
        "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
        "                          loss=losses,\n",
        "                          grad_norm=grad_norm,\n",
        "                          lr=scheduler.get_lr()[0]))\n",
        "        if CFG.wandb:\n",
        "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
        "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    losses = AverageMeter()\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    targets = []\n",
        "    start = end = time.time()\n",
        "    for step, batch in enumerate(valid_loader):\n",
        "        precipitation = batch['precipitation'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        with torch.no_grad():\n",
        "\n",
        "            pred_precipitation = model(precipitation)\n",
        "\n",
        "            #loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n",
        "            loss = criterion(pred_precipitation.squeeze(1), labels)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        preds.append(pred_precipitation.sigmoid().flatten().detach().cpu().numpy())\n",
        "        targets.append(labels.flatten().detach().cpu().numpy())\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(step, len(valid_loader),\n",
        "                          loss=losses,\n",
        "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
        "    predictions = np.concatenate(preds)\n",
        "    targets = np.concatenate(targets)\n",
        "    return losses.avg, predictions, targets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:10.106577Z",
          "iopub.execute_input": "2025-02-16T16:42:10.106838Z",
          "iopub.status.idle": "2025-02-16T16:42:10.126162Z",
          "shell.execute_reply.started": "2025-02-16T16:42:10.106819Z",
          "shell.execute_reply": "2025-02-16T16:42:10.125499Z"
        },
        "papermill": {
          "duration": 0.019437,
          "end_time": "2025-02-03T17:10:32.035131",
          "exception": false,
          "start_time": "2025-02-03T17:10:32.015694",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "TV1eVYe0jkVy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Loop"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.004977,
          "end_time": "2025-02-03T17:10:32.045348",
          "exception": false,
          "start_time": "2025-02-03T17:10:32.040371",
          "status": "completed"
        },
        "tags": [],
        "id": "ID8DoY7EjkVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# train loop\n",
        "# ====================================================\n",
        "def train_loop(folds, fold):\n",
        "\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # ====================================================\n",
        "    # loader\n",
        "    # ====================================================\n",
        "    train_folds = folds[(folds['fold'] != fold)].reset_index(drop=True)\n",
        "\n",
        "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = CustomDataset(train_folds, augment=True, mode='train')\n",
        "    valid_dataset = CustomDataset(valid_folds, augment=False, mode=\"train\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "    # ====================================================\n",
        "    # model & optimizer\n",
        "    # ====================================================\n",
        "\n",
        "    model = ResNet1d(stage_sizes=[2, 3, 3], num_filters=16).to(device)\n",
        "    model.to(device)\n",
        "    if CFG.t4_gpu:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    def build_optimizer(cfg, optimizer_parameters, device):\n",
        "        lr = cfg.lr\n",
        "        # lr = default_configs[\"lr\"]\n",
        "        if cfg.optimizer == \"SAM\":\n",
        "            base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n",
        "            optimizer_model = SAM(model.parameters(), base_optimizer, lr=lr, momentum=0.9, weight_decay=cfg.weight_decay, adaptive=True)\n",
        "        elif cfg.optimizer == \"Ranger21\":\n",
        "            optimizer_model = Ranger21(model.parameters(), lr=lr, weight_decay=cfg.weight_decay,\n",
        "            num_epochs=cfg.epochs, num_batches_per_epoch=len(train_loader))\n",
        "        elif cfg.optimizer == \"SGD\":\n",
        "            optimizer_model = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=cfg.weight_decay, momentum=0.9)\n",
        "        elif cfg.optimizer == \"Adam\":\n",
        "            optimizer_model = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n",
        "        elif cfg.optimizer == \"AdamW\":\n",
        "            optimizer_model = AdamW(optimizer_parameters, lr=lr)\n",
        "        elif cfg.optimizer == \"Lion\":\n",
        "            optimizer_model = Lion(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n",
        "        elif cfg.optimizer == \"Adan\":\n",
        "            optimizer_model = Adan(optimizer_parameters, lr=lr)\n",
        "\n",
        "        return optimizer_model\n",
        "\n",
        "    def get_optimizer_params(model, lr, weight_decay=0.0):\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "        optimizer_parameters = [\n",
        "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'lr': lr, 'weight_decay': weight_decay},\n",
        "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "             'lr': lr, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        return optimizer_parameters\n",
        "\n",
        "    optimizer_parameters = get_optimizer_params(model,\n",
        "                                                CFG.lr,\n",
        "                                                weight_decay=CFG.weight_decay)\n",
        "\n",
        "    optimizer = build_optimizer(CFG, optimizer_parameters, device)\n",
        "\n",
        "    # ====================================================\n",
        "    # scheduler\n",
        "    # ====================================================\n",
        "    # ====================================================\n",
        "\n",
        "    def get_scheduler(optimizer):\n",
        "        if CFG.scheduler=='ReduceLROnPlateau':\n",
        "            scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n",
        "        elif CFG.scheduler=='CosineAnnealingLR':\n",
        "            scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n",
        "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.cosanneal_res_params)\n",
        "        elif CFG.scheduler=='OneCycleLR':\n",
        "            steps_per_epoch=len(train_loader),\n",
        "            scheduler = OneCycleLR(optimizer=optimizer, epochs=CFG.epochs, anneal_strategy=\"cos\", pct_start=0.05, steps_per_epoch=len(train_loader),\n",
        "        max_lr=CFG.lr, final_div_factor=100)\n",
        "        return scheduler\n",
        "\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "\n",
        "    # ====================================================\n",
        "    # loop\n",
        "    # ====================================================\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "    best_score = np.inf\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # train\n",
        "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
        "\n",
        "        # eval\n",
        "        avg_val_loss, predictions, targets = valid_fn(valid_loader, model, criterion, device)\n",
        "        score = get_score(predictions, targets)\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  logloss: {score}  time: {elapsed:.0f}s')\n",
        "        if CFG.wandb:\n",
        "            wandb.log({f\"[fold{fold}] epoch\": epoch+1,\n",
        "                       f\"[fold{fold}] avg_train_loss\": avg_loss,\n",
        "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
        "                       f\"[fold{fold}] score\": score})\n",
        "\n",
        "        if best_score > score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best valid logloss: {score:.7f} Model')\n",
        "            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n",
        "\n",
        "            state_dict = model.module.state_dict() if CFG.t4_gpu else model.state_dict()\n",
        "            torch.save({'model': state_dict,\n",
        "                            'predictions': predictions},\n",
        "                             OUTPUT_DIR+f\"{CFG.model_name}_fold{fold}_best_version{VERSION}.pth\")\n",
        "\n",
        "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model_name}_fold{fold}_best_version{VERSION}.pth\",\n",
        "                             map_location=torch.device('cpu'))['predictions']\n",
        "\n",
        "    valid_folds[\"resnet1d_oof_preds\"] = predictions\n",
        "    torch.cuda.empty_cache()\n",
        "    for i in range(100):\n",
        "        _ = gc.collect()\n",
        "\n",
        "    return valid_folds, best_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:10.127036Z",
          "iopub.execute_input": "2025-02-16T16:42:10.12728Z",
          "iopub.status.idle": "2025-02-16T16:42:10.147314Z",
          "shell.execute_reply.started": "2025-02-16T16:42:10.127253Z",
          "shell.execute_reply": "2025-02-16T16:42:10.146466Z"
        },
        "papermill": {
          "duration": 0.019612,
          "end_time": "2025-02-03T17:10:32.070271",
          "exception": false,
          "start_time": "2025-02-03T17:10:32.050659",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "fiAAuc6ojkVz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    if CFG.train:\n",
        "        if CFG.wandb:\n",
        "            run = wandb.init(project='Inundata competition',\n",
        "                         name=CFG.model_name + f'version{VERSION}',\n",
        "                         config=class2dict(CFG),\n",
        "                         group=CFG.model_name,\n",
        "                         job_type=\"train\",\n",
        "                         )\n",
        "\n",
        "        oof_df = pd.DataFrame()\n",
        "        scores = []\n",
        "        for fold in range(CFG.n_fold):\n",
        "            if fold in CFG.trn_fold:\n",
        "                _oof_df, score = train_loop(data, fold)\n",
        "                oof_df = pd.concat([oof_df, _oof_df])\n",
        "                scores.append(score)\n",
        "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "                LOGGER.info(f'Score with best logloss weights: {score}')\n",
        "        oof_df = oof_df.reset_index(drop=True)\n",
        "        LOGGER.info(f\"========== CV ==========\")\n",
        "        LOGGER.info(f'Score with best logloss weights: {np.mean(scores)}')\n",
        "        oof_df.to_csv(OUTPUT_DIR+f'{CFG.model_name}_oof_df_version{VERSION}.csv', index=False)\n",
        "\n",
        "    if CFG.wandb:\n",
        "        wandb.finish()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-16T16:42:10.148163Z",
          "iopub.execute_input": "2025-02-16T16:42:10.148402Z",
          "iopub.status.idle": "2025-02-16T16:45:30.75682Z",
          "shell.execute_reply.started": "2025-02-16T16:42:10.148364Z",
          "shell.execute_reply": "2025-02-16T16:45:30.755933Z"
        },
        "papermill": {
          "duration": 4369.52714,
          "end_time": "2025-02-03T18:23:21.602615",
          "exception": false,
          "start_time": "2025-02-03T17:10:32.075475",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "f0WcA4HzjkVz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalizing the predictions based on the Flood Probaility"
      ],
      "metadata": {
        "id": "DrTQWj0Njl2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "print(f\"logloss before normalizing: {log_loss(oof_df['label'], oof_df['resnet1d_oof_preds'])}\")\n",
        "\n",
        "locations_to_normalize = oof_df[oof_df['flood_probability'] >= 0.5]['location_id'].unique()\n",
        "oof_df['oof_sum_prob'] = oof_df.groupby('location_id')['resnet1d_oof_preds'].transform('sum')\n",
        "\n",
        "# Avoid division by zero\n",
        "epsilon = 1e-8\n",
        "oof_df['oof_resnet1d_norm'] = oof_df['resnet1d_oof_preds']  # Copy original values\n",
        "\n",
        "oof_df.loc[oof_df['location_id'].isin(locations_to_normalize), 'oof_resnet1d_norm'] = (\n",
        "    oof_df.loc[oof_df['location_id'].isin(locations_to_normalize), 'resnet1d_oof_preds'] /\n",
        "    (oof_df.loc[oof_df['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n",
        ")\n",
        "\n",
        "print(f\"logloss after normalizing: {log_loss(oof_df['label'], oof_df['oof_resnet1d_norm'])}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T16:45:30.757943Z",
          "iopub.execute_input": "2025-02-16T16:45:30.758275Z",
          "iopub.status.idle": "2025-02-16T16:45:30.80955Z",
          "shell.execute_reply.started": "2025-02-16T16:45:30.758244Z",
          "shell.execute_reply": "2025-02-16T16:45:30.808739Z"
        },
        "id": "ySSkaji3jkVz"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}