{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10776345,
          "sourceType": "datasetVersion",
          "datasetId": 6682996
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import log_loss\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "import gc"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "2nTGJN6dhqfl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai -q"
      ],
      "metadata": {
        "trusted": true,
        "id": "86VGVwdYhqfr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "adY081YIhqft"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.tabular.all import *"
      ],
      "metadata": {
        "trusted": true,
        "id": "_fJVj_0Whqft"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"\n",
        "additional_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/image_classifier_results/\"\n",
        "train = pd.read_csv(base_path + \"Train.csv\")\n",
        "test = pd.read_csv(base_path + \"Test.csv\")\n",
        "train_with_cv_results = pd.read_csv(additional_path + \"train_with_cv_results.csv\")[['location_id', 'flood_probability']]\n",
        "test_with_cv_results = pd.read_csv(additional_path + \"test_with_cv_results.csv\")[['location_id', 'flood_probability',]]\n",
        "submission = pd.read_csv(base_path + \"SampleSubmission.csv\")\n",
        "images = np.load(base_path + \"composite_images.npz\")\n",
        "display(train.head(), train.shape, train_with_cv_results.head(), train_with_cv_results.shape, test.head(), test.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "c0Bhu7Ifhqfv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_location(value):\n",
        "  return value.split(\"_\")[0] + '_' + value.split(\"_\")[1]\n",
        "\n",
        "def get_event_id(value):\n",
        "  return value.split(\"_\")[3]\n",
        "for df in [train, test]:\n",
        "\n",
        "  df['location_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n",
        "  df['event_idx'] = df.groupby('location_id', sort=False).ngroup()\n",
        "\n",
        "  df['event_t'] = df.groupby('location_id').cumcount()\n",
        "\n",
        "print(len(set(train['location_id'])), len(set(test['location_id'])))\n",
        "print(len(set(train['location_id']).intersection(set(test['location_id']))))\n",
        "display(train.head(), test.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "kbCDOL4Vhqfw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.merge(train, train_with_cv_results, on='location_id', how='left')\n",
        "test_df = pd.merge(test, test_with_cv_results, on='location_id', how='left')\n",
        "\n",
        "\n",
        "display(train_df.head(), train_df.shape, test_df.head(), test_df.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AdylAhWxhqfw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 2024\n",
        "selected_columns = []\n",
        "n_splits = 10\n",
        "gkf = StratifiedGroupKFold(n_splits = n_splits)\n",
        "\n",
        "train_df['fold'] = -1\n",
        "for fold, (_, val_idx) in enumerate(gkf.split(train_df, train_df['label'], groups = train_df['location_id'])):\n",
        "    train_df.loc[val_idx, \"fold\"] = fold\n",
        "\n",
        "train_df['fold'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_uws7ZPRhqfx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from types import new_class\n",
        "def apply_expanding_combinations(df, group_cols_list, target_col='Sales', shift_periods=[1], min_periods=1, stats=['mean', 'std']):\n",
        "    # Loop through the group column combinations\n",
        "    for group_cols in group_cols_list:\n",
        "        # Generate base name for the grouping\n",
        "        group_name = '_'.join(group_cols)\n",
        "\n",
        "        for shift_period in shift_periods:\n",
        "            for stat in stats:\n",
        "                expanding_col_name = f'expanding_grouped_{group_name}_{target_col}_shift_{shift_period}_{stat}'\n",
        "\n",
        "                # Apply groupby, shift, and expanding for the given statistic\n",
        "                df[expanding_col_name] = df.groupby(group_cols)[target_col].transform(\n",
        "                    lambda x: x.shift(shift_period).expanding(min_periods=min_periods).agg(stat)\n",
        "                )\n",
        "\n",
        "    return df\n",
        "\n",
        "def smoothen_target(df, group_cols, target_col):\n",
        "  n_std = 10\n",
        "  for i_smooth in [target_col]:\n",
        "      df_id_outlier = df.groupby(group_cols,as_index=False).agg({\n",
        "          f'{i_smooth}': lambda x: x.mean() + n_std*x.std()\n",
        "      }).rename(columns={f'{i_smooth}':f'{i_smooth}_outlier'})\n",
        "\n",
        "      df_id_mean = df.groupby(group_cols,as_index=False).agg({\n",
        "          f'{i_smooth}': 'mean'\n",
        "      }).rename(columns={f'{i_smooth}':f'{i_smooth}_mean'})\n",
        "\n",
        "      df = df.merge(df_id_outlier, on=group_cols[0], how='left')\n",
        "      df = df.merge(df_id_mean, on=group_cols[0], how='left')\n",
        "\n",
        "      df[f'{i_smooth}'] = np.where(\n",
        "          df[f'{i_smooth}'] > df[f'{i_smooth}_outlier'],\n",
        "          df[f'{i_smooth}_mean'],\n",
        "          df[f'{i_smooth}']\n",
        "      )\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def create_rolling_features(data, group_cols, target_col, windows, shift_period, min_period, statistics):\n",
        "    def apply_statistic(x, stat):\n",
        "        rolled = x.shift(shift_period).rolling(window=window, min_periods=min_period)\n",
        "        if stat == 'mean':\n",
        "            return rolled.mean()\n",
        "        elif stat == 'median':\n",
        "            return rolled.median()\n",
        "        elif stat == 'std':\n",
        "            return rolled.std()\n",
        "        elif stat == 'min':\n",
        "            return rolled.min()\n",
        "        elif stat == 'max':\n",
        "            return rolled.max()\n",
        "        elif stat == 'skew':\n",
        "            return rolled.skew()\n",
        "        elif stat == 'sum':\n",
        "            return rolled.sum()\n",
        "        elif stat == 'quantile':\n",
        "            return rolled.quantile(0.95)\n",
        "\n",
        "        elif stat.startswith('quantile_'):\n",
        "            q = float(stat.split('_')[1])\n",
        "            return rolled.quantile(q)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown statistic: {stat}\")\n",
        "\n",
        "    for window in windows:\n",
        "        for stat in statistics:\n",
        "            stat_name = stat if not stat.startswith('quantile_') else f\"{stat.split('_')[1]}th\"\n",
        "            col_name = f'rolling_previous_grouped_{target_col}_{stat_name}_{window}_{shift_period}'\n",
        "\n",
        "            data[col_name] = data.groupby(group_cols)[target_col].transform(\n",
        "                lambda x: apply_statistic(x, stat)\n",
        "            )\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def custom_agg(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "def get_date_features(df):\n",
        "  # Simulate year (assuming 365 days per year)\n",
        "  df['year'] = (df['event_t'] // 365) + 1  # Year 1 or 2\n",
        "\n",
        "  # Simulate month (approximate)\n",
        "  df['month'] = ((df['event_t'] % 365) // 30) + 1  # 30-day months approximation\n",
        "\n",
        "  # Simulate week of the year\n",
        "  df['week_of_year'] = (df['event_t'] % 365) // 7 + 1\n",
        "\n",
        "  # Simulate day of the month\n",
        "  df['day_of_month'] = (df['event_t'] % 30) + 1  # Assuming 30-day months\n",
        "\n",
        "  # Simulate day of the week (0 = Monday, 6 = Sunday)\n",
        "  df['day_of_week'] = df['event_t'] % 7\n",
        "\n",
        "  # Simulate quarter\n",
        "  df['quarter'] = ((df['month'] - 1) // 3) + 1\n",
        "  return  df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def feature_engineering(train, test):\n",
        "  data = pd.concat([train, test])\n",
        "  data.sort_values(by = ['location_id', 'event_t'], inplace=True)\n",
        "  data['event_t'] = data['event_t'].astype(int)\n",
        "  # data = smoothen_target(data, ['location_id'], 'precipitation')\n",
        "\n",
        "  data['event_binary'] = data['event_t'].apply(lambda x: 1 if (x >= 296 and x <= 435) else 0)\n",
        "\n",
        "\n",
        "  group_cols =['location_id']\n",
        "  # data = apply_expanding_combinations(\n",
        "  #     data,\n",
        "  #     [group_cols],\n",
        "  #     target_col='precipitation',\n",
        "  #     shift_periods=[1],#1,3, 4, 5, 6, 7, 8, 24\n",
        "  #     min_periods=1,\n",
        "  #     stats=['mean']\n",
        "  # )\n",
        "\n",
        "  statistics = ['mean'] #, 'median', 'std', 'quantile_0.25', 'quantile_0.75'\n",
        "  min_period = 1\n",
        "\n",
        "  shift_period = 0\n",
        "  windows = [3, 4,10,20, 25, 30,55,60, 75, 296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 2\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 3\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 4\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 5\n",
        "  # windows = [3, 4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 6\n",
        "  # windows = [ 3,4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "\n",
        "  # shift_period = 8\n",
        "  # windows = [ 3,4,10,20, 25, 30,55,60, 75,296]#3, 4, 10, 20,25,30, 50, 55, 60, 75,\n",
        "  # data = create_rolling_features(data, group_cols,'precipitation', windows, shift_period, min_period, statistics)\n",
        "  # # data = get_date_features(data)\n",
        "\n",
        "  for col in ['precipitation']:\n",
        "    # data[f\"grouped_location_{col}_cum\"] = data.groupby('location_id')[col].cumsum().shift(1)\n",
        "\n",
        "    # quantile = 0.95  # Define the quantile you want to calculate\n",
        "    # for stat in ['mean', 'quantile']:\n",
        "    #     if stat != 'quantile':\n",
        "    #         data[f\"location_grouped_{col}_{stat}\"] = data.groupby('location_id')[col].transform(stat)\n",
        "    #         data[f\"diff_{col}_{stat}\"] = data[col] - data[f\"location_grouped_{col}_{stat}\"]\n",
        "\n",
        "\n",
        "    for shift in range(1,365):\n",
        "      data[f'{col}_shift_{shift}'] = data.groupby('location_id')[col].shift(shift)\n",
        "      data[f'{col}_next_shift_{shift}'] = data.groupby('location_id')[col].shift(-shift)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # for window in windows:\n",
        "    #   data[f'{col}_rolling_grouped_custom_{window}'] = (\n",
        "    #       data.groupby('location_id')[col]\n",
        "    #       .rolling(window)\n",
        "    #       .apply(custom_agg)\n",
        "    #       .reset_index(level=0, drop=True)  # Reset the index to align with the original DataFrame\n",
        "    #   )\n",
        "\n",
        "    for span in [7]:\n",
        "        data[f'{col}_ewm_grouped_mean_{span}'] = (\n",
        "            data.groupby('location_id')[col]\n",
        "            .ewm(span=span, adjust=False)\n",
        "            .mean()\n",
        "            .reset_index(level=0, drop=True)  # Reset the index to align it with the original DataFrame\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  train = data[data['label'].notna()].reset_index(drop = True)\n",
        "  test = data[data['label'].isna()].reset_index(drop = True)\n",
        "\n",
        "  return train, test\n",
        "\n",
        "new_train, new_test = feature_engineering(train_df, test_df)\n",
        "display(new_train.head(), new_train.shape, new_test.head(), new_test.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "X3-v6IVjhqfy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "new_train['label'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "AwpMSKY1hqfz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_splits):\n",
        "  print(new_train[new_train['fold'] == i]['label'].value_counts())\n",
        "  print(\"-\"* 100)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TuiaGRAmhqf0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns =['precipitation','flood_probability','event_binary', 'event_t', ] + [col for col in new_train if 'diff' in col or 'shift' in col or 'grouped' in col ]\n",
        "target_col = 'label'"
      ],
      "metadata": {
        "trusted": true,
        "id": "NOWgcaPyhqf0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "len(selected_columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZLFo0Vxdhqf1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def random_seed(seed_value, use_cuda):\n",
        "    np.random.seed(seed_value)\n",
        " #cpu vars\n",
        "    torch.manual_seed(seed_value)\n",
        "# cpu  vars\n",
        "    random.seed(seed_value)\n",
        " # Python\n",
        "    if use_cuda:\n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "# gpu vars\n",
        "        torch.backends.cudnn.deterministic = True\n",
        " #needed\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "#Remember to use num_workers=0 when creating the DataBunch.\n",
        "\n",
        "random_seed(42,True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Tit8aYdshqf1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class TabTransformer(nn.Module):\n",
        "    def __init__(self, num_features=743, num_classes=1, dim_embedding=96, num_heads=4, num_layers=2):\n",
        "        super(TabTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(num_features, dim_embedding)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(dim_embedding, num_classes)\n",
        "\n",
        "    def forward(self, _, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.unsqueeze(1)  # Adding a sequence length dimension\n",
        "        x = self.transformer(x)\n",
        "        x = torch.mean(x, dim=1)  # Pooling\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ezPbLdv_hqf1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "IHZ3dM33hqf2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_fastai_model(train, test, target_col, selected_columns, n_splits):\n",
        "    train['oof_preds'] = 0.0  # Initialize OOF predictions in train\n",
        "    test_preds = np.zeros(len(test), dtype=np.float32)  # Initialize test predictions\n",
        "    scores_auc = []  # Store AUC scores\n",
        "    scores_logloss = []  # Store Log Loss scores\n",
        "\n",
        "    cat_feats = []  # Categorical features\n",
        "    cont_feats = [col for col in selected_columns if col not in cat_feats]  # Continuous features\n",
        "\n",
        "    for fold in range(n_splits):\n",
        "        print(\"*\" * 100)\n",
        "        print(f\"======================================TRAINING FOLD: {fold}=============================================\")\n",
        "        model = TabTransformer()\n",
        "        # Split train into training and validation sets\n",
        "        training = train[train['fold'] != fold]\n",
        "        validation = train[train['fold'] == fold]\n",
        "\n",
        "        splits = (\n",
        "            list(range(len(training))),\n",
        "            list(range(len(training), len(training) + len(validation)))\n",
        "        )\n",
        "\n",
        "        combined_data = pd.concat(\n",
        "            [training[selected_columns + [target_col]], validation[selected_columns + [target_col]]]\n",
        "        )\n",
        "\n",
        "        # Prepare DataLoaders\n",
        "        dls = TabularPandas(\n",
        "            combined_data,\n",
        "            cat_names=cat_feats,\n",
        "            cont_names=cont_feats,\n",
        "            y_names=target_col,\n",
        "            splits=splits,\n",
        "            procs=[Categorify, FillMissing, Normalize]\n",
        "        ).dataloaders(bs=4096*2)\n",
        "\n",
        "        # Define the model as a binary classifier\n",
        "        #learn = tabular_learner(\n",
        "        #    dls,\n",
        "        #    layers= [256],#[ 256,512, 1024, 512, 256],\n",
        "        #    n_out=1,\n",
        "        #    loss_func=F.binary_cross_entropy_with_logits,\n",
        "        #    metrics=[AccumMetric(roc_auc_score, invert_arg=True)]\n",
        "        #)\n",
        "        learn = Learner(\n",
        "                    dls,\n",
        "                    model=model,\n",
        "                    metrics=[AccumMetric(roc_auc_score, invert_arg=True)],\n",
        "                    loss_func=F.binary_cross_entropy_with_logits,\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        learn.fit_one_cycle(25, 1e-3, cbs=[SaveModelCallback(monitor='valid_loss', fname=f'nn_approach_fold_{fold}')])\n",
        "\n",
        "        # Validation predictions\n",
        "        val_dl = learn.dls.test_dl(validation[selected_columns])\n",
        "        preds, _ = learn.get_preds(dl=val_dl)\n",
        "        val_preds = preds.sigmoid().squeeze().numpy()  # Sigmoid for probabilities\n",
        "        auc_score = roc_auc_score(validation[target_col], val_preds)\n",
        "        logloss_score = log_loss(validation[target_col], val_preds)\n",
        "\n",
        "        scores_auc.append(auc_score)\n",
        "        scores_logloss.append(logloss_score)\n",
        "\n",
        "        print(f\"Fold {fold} AUC: {auc_score:.4f}, LogLoss: {logloss_score:.4f}\")\n",
        "\n",
        "        # Assign OOF predictions to train\n",
        "        train.loc[validation.index, 'oof_preds'] = val_preds\n",
        "\n",
        "        # Test predictions\n",
        "        test_dl = learn.dls.test_dl(test[selected_columns])\n",
        "        preds, _ = learn.get_preds(dl=test_dl)\n",
        "        test_preds += preds.sigmoid().squeeze().numpy()\n",
        "\n",
        "        # Cleanup\n",
        "        del dls, learn, val_dl, test_dl, preds, _\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Combine test predictions (mean across folds)\n",
        "    combined_test_preds = test_preds / n_splits\n",
        "\n",
        "    print(f\"\\nAverage AUC across {n_splits} folds: {np.mean(scores_auc):.4f} (+/- {np.std(scores_auc):.4f})\")\n",
        "    print(f\"Average LogLoss across {n_splits} folds: {np.mean(scores_logloss):.4f} (+/- {np.std(scores_logloss):.4f})\")\n",
        "\n",
        "    # Overall scores for OOF\n",
        "    overall_auc = roc_auc_score(train[target_col], train['oof_preds'])\n",
        "    overall_logloss = log_loss(train[target_col], train['oof_preds'])\n",
        "    print(f\"Overall OOF AUC: {overall_auc:.4f}, LogLoss: {overall_logloss:.4f}\")\n",
        "\n",
        "    # Assign combined test predictions to the test set\n",
        "    test['preds'] = combined_test_preds\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "trusted": true,
        "id": "6PCj24ybhqf2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 10\n",
        "sub_train, sub_test =  fit_fastai_model(new_train, new_test, target_col, selected_columns, n_splits)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tr3KUGrLhqf3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "display(sub_test.head(), sub_test.shape)"
      ],
      "metadata": {
        "trusted": true,
        "id": "rUED9A27hqf3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sub = sub_test[['event_id', 'preds']]\n",
        "sub.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "lUl01NwXhqf4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sub.to_csv(\"baseline_fastai_tabtransformer_10_folds.csv\", index = False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7mNaJh18hqf4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sub_train[['event_id','label', 'oof_preds']].to_csv(\"oof.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZNPlzczdhqf4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sub_train[['event_id', 'location_id', 'event_t', 'flood_probability','label','oof_preds', ]].to_csv(\"transformer_tab_oof.csv\", index=False)\n",
        "sub_test[['event_id', 'location_id', 'event_t', 'flood_probability','label', 'preds']].to_csv(\"transformer_tab_test.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ill5NiRlhqf4"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}