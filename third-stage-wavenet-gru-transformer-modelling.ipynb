{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9984259,"sourceType":"datasetVersion","datasetId":6144078},{"sourceId":10716224,"sourceType":"datasetVersion","datasetId":6642380},{"sourceId":10776345,"sourceType":"datasetVersion","datasetId":6682996}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Directory Settings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# directory settings\n# ====================================================\n\nimport os\n\nOUTPUT_DIR = './'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:53:50.195982Z","iopub.execute_input":"2025-02-17T16:53:50.196231Z","iopub.status.idle":"2025-02-17T16:53:50.200565Z","shell.execute_reply.started":"2025-02-17T16:53:50.196213Z","shell.execute_reply":"2025-02-17T16:53:50.199724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom glob import glob\nimport sys\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom scipy.stats import entropy\nfrom scipy.signal import butter, lfilter, freqz\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom tqdm.auto import tqdm\nfrom functools import partial\nimport cv2\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR, CosineAnnealingLR, CosineAnnealingWarmRestarts\nfrom sklearn.preprocessing import LabelEncoder\nfrom torchvision.transforms import v2\nfrom sklearn.model_selection import GroupKFold, StratifiedGroupKFold\nfrom sklearn.model_selection import train_test_split\nimport albumentations as A\nfrom albumentations import (Compose, Normalize, Resize, RandomResizedCrop, HorizontalFlip, VerticalFlip, ShiftScaleRotate, Transpose)\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\nimport timm\nimport warnings \nwarnings.filterwarnings('ignore')\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom matplotlib import pyplot as plt\nimport joblib\nVERSION=9\nbase_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/zindi_data/\"\nadditional_path = \"/kaggle/input/final-deepmind-comp-dataset/final_deepmind_comp_dataset/image_classifier_results/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:58:11.864174Z","iopub.execute_input":"2025-02-17T16:58:11.864541Z","iopub.status.idle":"2025-02-17T16:58:11.873060Z","shell.execute_reply.started":"2025-02-17T16:58:11.864512Z","shell.execute_reply":"2025-02-17T16:58:11.872210Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\n\nclass CFG:\n    wandb = False\n    debug = False\n    train=True\n    apex=True\n    t4_gpu=True\n    scheduler='OneCycleLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts','OneCycleLR']\n    # CosineAnnealingLR params\n    cosanneal_params={\n        'T_max':6,\n        'eta_min':1e-5,\n        'last_epoch':-1\n    }\n    #ReduceLROnPlateau params\n    reduce_params={\n        'mode':'min',\n        'factor':0.2,\n        'patience':4,\n        'eps':1e-6,\n        'verbose':True\n    }\n    # CosineAnnealingWarmRestarts params\n    cosanneal_res_params={\n        'T_0':20,\n        'eta_min':1e-6,\n        'T_mult':1,\n        'last_epoch':-1\n    }\n    print_freq=8\n    num_workers = 1\n    model_name = 'wavenet_gru_transformer'\n    optimizer='Adan'\n    epochs = 25\n    factor = 0.9\n    patience = 2\n    eps = 1e-6\n    lr = 1e-3\n    min_lr = 1e-6\n    batch_size = 32\n    weight_decay = 1e-2\n    batch_scheduler=True\n    gradient_accumulation_steps = 1\n    max_grad_norm = 1e6\n    seed = 2025\n    target_cols = \"label\"\n    target_size = 1\n    in_channels = 1\n    n_fold = 10\n    # trn_fold = [2]\n    trn_fold = [i for i in range(n_fold)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:54:07.780237Z","iopub.execute_input":"2025-02-17T16:54:07.780617Z","iopub.status.idle":"2025-02-17T16:54:07.786337Z","shell.execute_reply.started":"2025-02-17T16:54:07.780587Z","shell.execute_reply":"2025-02-17T16:54:07.785537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\ndef get_score(preds, targets):\n    \n    return log_loss(targets, preds)\n\n\nclass ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        output1_norm = F.normalize(output1)\n        output2_norm = F.normalize(output2)\n        euclidean_distance = F.cosine_similarity(output1_norm, output2_norm)\n        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n        return loss_contrastive\n\ndef get_location(value):\n  return value.split(\"_\")[0] + '_' + value.split(\"_\")[1]\n\n\n\n# def seed_torch(seed=42):\n#     random.seed(seed)\n#     os.environ['PYTHONHASHSEED'] = str(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.cuda.manual_seed_all(seed)  # If using multi-GPU\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = False\n\ndef seed_torch(seed=CFG.seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.enabled = True\n    \nseed_torch(seed=CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:54:12.345513Z","iopub.execute_input":"2025-02-17T16:54:12.345801Z","iopub.status.idle":"2025-02-17T16:54:12.366364Z","shell.execute_reply.started":"2025-02-17T16:54:12.345779Z","shell.execute_reply":"2025-02-17T16:54:12.365584Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"markdown","source":"## Load precipitation data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(base_path + \"Train.csv\")\ndata_test = pd.read_csv(base_path + \"Test.csv\")\ndata_with_cv = pd.read_csv(additional_path + \"train_with_cv_results.csv\")[['location_id', 'flood_probability']]\ndata_test_with_cv = pd.read_csv(additional_path + \"test_with_cv_results.csv\")[['location_id', 'flood_probability',]]\n\nfor df in [data, data_test]:\n\n  df['location_id'] = df['event_id'].apply(lambda x: '_'.join(x.split('_')[0:2]))\n  df['event_idx'] = df.groupby('location_id', sort=False).ngroup()\n\n  df['event_t'] = df.groupby('location_id').cumcount()\n\n\ndata = pd.merge(data, data_with_cv, on='location_id', how='left')\ndata_test = pd.merge(data_test, data_test_with_cv, on='location_id', how='left')\n\n\nprint(len(set(data['location_id'])), len(set(data_test['location_id'])))\nprint(len(set(data['location_id']).intersection(set(data_test['location_id']))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:59:41.144080Z","iopub.execute_input":"2025-02-17T16:59:41.144442Z","iopub.status.idle":"2025-02-17T16:59:42.002090Z","shell.execute_reply.started":"2025-02-17T16:59:41.144379Z","shell.execute_reply":"2025-02-17T16:59:42.000240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:59:51.607659Z","iopub.execute_input":"2025-02-17T16:59:51.607943Z","iopub.status.idle":"2025-02-17T16:59:51.625946Z","shell.execute_reply.started":"2025-02-17T16:59:51.607918Z","shell.execute_reply":"2025-02-17T16:59:51.624861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gkf = StratifiedGroupKFold(n_splits=CFG.n_fold)\n\ndata['fold'] = -1\n\nfor fold_id, (_, val_idx) in enumerate(\n    gkf.split(data, y=data['label'], groups=data['location_id'])\n):\n    print(f\"Fold {fold_id}\")\n    print(f\"group={data['location_id'][val_idx].unique()}\")\n    data.loc[val_idx, 'fold'] = fold_id\n    \ndel gkf\n_ = gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:59:51.812115Z","iopub.execute_input":"2025-02-17T16:59:51.812462Z","iopub.status.idle":"2025-02-17T16:59:53.536287Z","shell.execute_reply.started":"2025-02-17T16:59:51.812432Z","shell.execute_reply":"2025-02-17T16:59:53.535477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def norm_feature_engineering(df):\n\n    # df = apply_boxcox_transformation(df, \"precipitation\")\n\n    df['precipitation'] = np.log(df[\"precipitation\"] + 0.01).astype(\"float32\")\n\n    \n    for w in range(1, 182):    \n        df['precipitation_shift_pos_' + str(w)] = df['precipitation'].shift(w).fillna(0)\n        df['precipitation_shift_neg_' + str(w)] = df['precipitation'].shift(-w).fillna(0)\n\n    \n    \n    \n    df = df[[col for col in df.columns if 'precipitation' in col] + ['flood_probability']].fillna(0)\n\n    return df.to_numpy().transpose(1, 0)\n\ndef apply_boxcox_transformation(df, col):\n    \"\"\"\n    Applies the Box-Cox transformation to a specified column while handling zero values.\n    \n    Args:\n        df (pd.DataFrame): Input DataFrame.\n        col (str): Column name to transform.\n    \n    Returns:\n        pd.DataFrame: DataFrame with the transformed column.\n    \"\"\"\n    df = df.copy()\n    pt = PowerTransformer(method='box-cox')\n    \n    # Mask for non-zero values\n    non_zero_mask = df[col] != 0.0\n    \n    # Initialize transformed data array\n    transformed_data = np.zeros_like(df[col], dtype=float)\n    \n    # Apply transformation to non-zero values\n    non_zero_transformed = pt.fit_transform(df.loc[non_zero_mask, [col]]).flatten()\n    \n    # Assign transformed values\n    min_value = non_zero_transformed.min()\n    transformed_data[non_zero_mask] = non_zero_transformed\n    transformed_data[~non_zero_mask] = min_value - 0.001  # Slightly below min\n    \n    # Assign back to DataFrame\n    df[col] = transformed_data\n    return df\n\ndef time_warp(features, sigma=0.2, knot=4):\n    \"\"\"\n    Apply time warping to each feature independently.\n    \"\"\"\n    warped_features = np.zeros_like(features)\n    orig_steps = np.arange(features.shape[1])\n\n    for i in range(features.shape[0]):  # Iterate over features (17)\n        random_warp = np.random.normal(loc=1.0, scale=sigma, size=(knot,))\n        interp = interp1d(np.linspace(0, features.shape[1], num=knot), random_warp, kind='linear', fill_value='extrapolate')\n        warped_series = interp(orig_steps)\n        warped_features[i] = np.interp(orig_steps * warped_series, orig_steps, features[i])\n\n    return warped_features\n\ndef time_shift(features, max_shift=10):\n    shift = np.random.randint(-max_shift, max_shift)\n    return np.roll(features, shift, axis=1)  # Shift along the time dimension\n\n\ndef add_gaussian_noise(features, std=0.1):\n    noise = np.random.normal(0, std, size=features.shape)\n    return features + noise\n\ndef feature_dropout(features, drop_prob=0.2):\n    mask = np.random.binomial(1, 1 - drop_prob, (features.shape[0], 1))  # Same mask for all time steps\n    return features * mask\n\ndef scale_features(features, scale_range=(0.8, 1.2)):\n    scales = np.random.uniform(scale_range[0], scale_range[1], size=(features.shape[0], 1))\n    return features * scales\n\ndef freq_perturbation(features, alpha=0.1):\n    fft_coeffs = np.fft.fft(features, axis=1)  # Apply FFT along time dimension\n    perturb = np.random.normal(1, alpha, size=fft_coeffs.shape)\n    return np.real(np.fft.ifft(fft_coeffs * perturb, axis=1))  # Apply inverse FFT\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(\n        self, df: pd.DataFrame, augment: bool = False, \n        mode: bool = True\n    ):\n        self.df = df\n        self.augment = augment\n        self.mode = mode\n        self.location_ids = df['location_id'].unique()\n\n        \n\n    def __len__(self):\n        return len(self.df['location_id'].unique())\n\n    def __getitem__(self, index):\n\n        precipitation, label = self.__data_generation(index)\n        \n        if self.augment:\n            if np.random.rand() < 0.3:\n                precipitation = time_shift(precipitation)\n\n            if np.random.rand() < 0.3:\n                precipitation = add_gaussian_noise(precipitation)\n            if np.random.rand() < 0.3:\n                precipitation = feature_dropout(precipitation)\n            \n        return {'precipitation': torch.tensor(precipitation, dtype=torch.float32), 'label': torch.tensor(label, dtype=torch.float32)}\n\n    def __data_generation(self, index):\n\n        event_id = self.location_ids[index]\n        # precipitation = self.df[self.df['event_id'] == event_id].pivot(index='event_id', columns='event_t', values='precipitation').to_numpy()\n        precipitation = self.df[self.df['location_id'] == event_id].sort_values('event_t')\n        precipitation = norm_feature_engineering(precipitation)\n        \n        label = np.zeros(730, dtype='float32')\n        if self.mode != 'test':\n            label = self.df[self.df['location_id'] == event_id].pivot(index='location_id', columns='event_t', values='label').to_numpy()\n            label = np.squeeze(label, axis=0)\n\n        return precipitation, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:59:56.068288Z","iopub.execute_input":"2025-02-17T16:59:56.068665Z","iopub.status.idle":"2025-02-17T16:59:56.081610Z","shell.execute_reply.started":"2025-02-17T16:59:56.068635Z","shell.execute_reply":"2025-02-17T16:59:56.080456Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# W&B Settings","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# wandb\n# ====================================================\nif CFG.wandb:\n    \n    import wandb\n\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        secret_value_0 = user_secrets.get_secret(\"wandb_key\")\n        wandb.login(key=secret_value_0)\n        anony = None\n    except:\n        anony = \"must\"\n        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n\n\n    def class2dict(f):\n        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:59:56.540310Z","iopub.execute_input":"2025-02-17T16:59:56.540649Z","iopub.status.idle":"2025-02-17T16:59:56.545355Z","shell.execute_reply.started":"2025-02-17T16:59:56.540624Z","shell.execute_reply":"2025-02-17T16:59:56.544496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class SinusoidalPositionalEmbedding(torch.nn.Module):\n    def __init__(self, seq_len, dim_model):\n        \"\"\"\n        Initialize the sinusoidal positional embedding.\n\n        Args:\n        seq_len (int): The length of the sequence (e.g., 730 days).\n        dim_model (int): The model's embedding size (e.g., 512).\n        \"\"\"\n        super(SinusoidalPositionalEmbedding, self).__init__()\n        self.seq_len = seq_len\n        self.dim_model = dim_model\n        self.positional_embedding = self._create_positional_embedding()\n\n    def _create_positional_embedding(self):\n        \"\"\"\n        Create the sinusoidal positional embedding tensor.\n\n        Returns:\n        torch.Tensor: Positional embeddings of shape (seq_len, dim_model).\n        \"\"\"\n        position = torch.arange(self.seq_len, dtype=torch.float).unsqueeze(1)  # Shape: (seq_len, 1)\n        div_term = torch.exp(\n            torch.arange(0, self.dim_model, 2, dtype=torch.float) *\n            (-math.log(10000.0) / self.dim_model)\n        )  # Frequencies: Shape (dim_model / 2)\n\n        # Compute sin and cos\n        pos_embedding = torch.zeros(self.seq_len, self.dim_model)\n        pos_embedding[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n        pos_embedding[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n\n        return pos_embedding\n\n    def forward(self, x):\n        \"\"\"\n        Add positional embedding to input tensor.\n\n        Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim_model).\n\n        Returns:\n        torch.Tensor: Input tensor with positional embeddings added.\n        \"\"\"\n        batch_size, seq_len, dim_model = x.size()\n        if seq_len != self.seq_len or dim_model != self.dim_model:\n            raise ValueError(\n                f\"Input shape mismatch: Expected (batch_size, {self.seq_len}, {self.dim_model}), got {x.size()}\"\n            )\n        return x + self.positional_embedding.to(x.device).unsqueeze(0)  # Broadcast positional embeddings\n\n\nclass Wave_Block(nn.Module):\n    def __init__(self, in_channels, out_channels, dilation_rates, kernel_size):\n        super(Wave_Block, self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n    \n        self.convs.append(nn.Conv1d(in_channels, out_channels, kernel_size=1))\n        dilation_rates = [2 ** i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(\n                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate, padding_mode='replicate'))\n            self.gate_convs.append(\n                nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, padding=int((dilation_rate*(kernel_size-1))/2), dilation=dilation_rate, padding_mode='replicate'))\n            self.convs.append(nn.Conv1d(out_channels, out_channels, kernel_size=1))\n\n    def forward(self, x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = torch.tanh(self.filter_convs[i](x)) * torch.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i + 1](x)\n            res = res + x\n        return res\n\nclass SEModule(nn.Module):\n    def __init__(self, in_channels, reduction=2):\n        super(SEModule, self).__init__()\n        self.conv = nn.Conv1d(in_channels, in_channels, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        s = F.adaptive_avg_pool1d(x, 1)\n        s = self.conv(s)\n        x *= torch.sigmoid(s)\n        return x\n\nclass WAVELT(nn.Module):\n\n    def __init__(self, inch=364, kernel_size=3):\n        super().__init__()\n        dropout_rate = 0.1\n    \n        self.conv1d_1 = nn.Conv1d(inch, 32, kernel_size=1, stride=1, dilation=1, padding=0, padding_mode='replicate')\n        self.batch_norm_conv_1 = nn.BatchNorm1d(32)\n        self.dropout_conv_1 = nn.Dropout(dropout_rate)\n    \n        self.conv1d_2 = nn.Conv1d(inch+16+32+64+128, 32, kernel_size=1, stride=1, dilation=1, padding=0, padding_mode='replicate')\n        self.batch_norm_conv_2 = nn.BatchNorm1d(32)\n        self.dropout_conv_2 = nn.Dropout(dropout_rate)\n    \n        self.wave_block1 = Wave_Block(32, 16, 12, kernel_size)\n        self.wave_block2 = Wave_Block(inch+16, 32, 8, kernel_size)\n        self.wave_block3 = Wave_Block(inch+16+32, 64, 4, kernel_size)\n        self.wave_block4 = Wave_Block(inch+16+32+64, 128, 1, kernel_size)\n    \n        self.se_module1 = SEModule(16)\n        self.se_module2 = SEModule(32)\n        self.se_module3 = SEModule(64)\n        self.se_module4 = SEModule(128)        \n    \n        self.batch_norm_1 = nn.BatchNorm1d(16)\n        self.batch_norm_2 = nn.BatchNorm1d(32)\n        self.batch_norm_3 = nn.BatchNorm1d(64)\n        self.batch_norm_4 = nn.BatchNorm1d(128)\n    \n        self.dropout_1 = nn.Dropout(dropout_rate)\n        self.dropout_2 = nn.Dropout(dropout_rate)\n        self.dropout_3 = nn.Dropout(dropout_rate)\n        self.dropout_4 = nn.Dropout(dropout_rate)\n    \n        self.gru1 = nn.GRU(32, 256, num_layers=1, batch_first=True, bidirectional=True)\n        self.gru2 = nn.GRU(32, 256, num_layers=1, batch_first=True, bidirectional=True)\n\n        self.project_embedding = nn.Linear(inch, 512)\n        self.pos_emb = SinusoidalPositionalEmbedding(730, 512)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=512, nhead=512//32, dim_feedforward=2*512,\n                dropout=0.1, activation=nn.GELU(), batch_first=True, norm_first=True), 2)\n        self.logit = nn.Conv1d(512, 1, kernel_size=7, padding=3, stride=1)\n\n    def extract_features(self, x):\n\n        project_precipitation = self.project_embedding(x.permute(0, 2, 1))\n    \n        x0 = self.conv1d_1(x)\n        x0 = F.relu(x0)\n        x0 = self.batch_norm_conv_1(x0)\n        x0 = self.dropout_conv_1(x0)\n    \n        x1 = self.wave_block1(x0)\n        x1 = self.batch_norm_1(x1)\n        x1 = self.dropout_1(x1)\n        x1 = self.se_module1(x1)\n        x2_base = torch.cat([x1, x], dim=1)\n    \n        x2 = self.wave_block2(x2_base)\n        x2 = self.batch_norm_2(x2)\n        x2 = self.dropout_2(x2)\n        x2 = self.se_module2(x2)\n        x3_base = torch.cat([x2_base, x2], dim=1)\n    \n        x3 = self.wave_block3(x3_base)\n        x3 = self.batch_norm_3(x3)\n        x3 = self.dropout_3(x3)\n        x3 = self.se_module3(x3)\n        x4_base = torch.cat([x3_base, x3], dim=1)\n    \n        x4 = self.wave_block4(x4_base)\n        x4 = self.batch_norm_4(x4)\n        x4 = self.dropout_4(x4)\n        x4 = self.se_module4(x4)\n        \n        x5_base = torch.cat([x4_base, x4], dim=1)\n        x5 = self.conv1d_2(x5_base)\n        x5 = F.relu(x5)\n        x5 = self.batch_norm_conv_2(x5)\n        x5 = self.dropout_conv_2(x5)\n    \n        gru_out1, hidden1 = self.gru1(x5.permute(0, 2, 1))\n        gru_out2, _ = self.gru2(x5.permute(0, 2, 1), hidden1)\n        pos_embedding = self.pos_emb(project_precipitation)\n        transformer_input = pos_embedding  + gru_out2\n        transformer_fusion = self.transformer(transformer_input)\n        \n        return transformer_fusion\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        output = self.extract_features(x)\n        out = self.logit(output.permute(0, 2, 1))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:32:02.096945Z","iopub.execute_input":"2025-02-16T05:32:02.097204Z","iopub.status.idle":"2025-02-16T05:32:02.117929Z","shell.execute_reply.started":"2025-02-16T05:32:02.097171Z","shell.execute_reply":"2025-02-16T05:32:02.117269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if CFG.debug:\n\n    train_data = data[data['fold'] == 0]\n    \n    dataset = CustomDataset(train_data, augment=True, mode='valid')\n    dataloader = DataLoader(dataset,\n                            batch_size=CFG.batch_size,\n                            shuffle=True,\n                            num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    \n    batch = next(iter(dataloader))\n\n    \n\n    model = WAVELT().to(device)\n\n    pred_precipitation = model(batch['precipitation'].to(device))\n\n\n    criteria = nn.BCEWithLogitsLoss()\n    loss = criteria(pred_precipitation.squeeze(1), batch['label'].to(device))\n    # contrastive_target = torch.zeros(image_emb.size(0)).to(device)  # Assuming all pairs are not similar\n    \n    # assert torch.all(torch.isfinite(image_emb)), \"Non-finite values in output1\"\n    # assert torch.all(torch.isfinite(precipitation_emb)), \"Non-finite values in output2\"\n    # assert torch.all((contrastive_target == 0) | (contrastive_target == 1)), \"Labels must be 0 or 1\"\n    # contrastive_loss = ContrastiveLoss()(image_emb, precipitation_emb, contrastive_target)\n\n    metric = log_loss(batch['label'].flatten().detach().cpu().numpy(), pred_precipitation.sigmoid().flatten().detach().cpu().numpy())\n    \n    # print(\"early_fusion: \", early_fusion.shape)\n    # print(\"pred_image: \", pred_image.shape)\n    print(\"pred_precipitation: \", pred_precipitation.shape)\n    # print(\"image_emb: \", image_emb.shape)\n    # print(\"precipitation_emb: \", precipitation_emb.shape)\n    print(\"loss: \", loss.item())\n    # print(\"contrastive loss: \", contrastive_loss.item())\n    print(\"metric: \", metric)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:32:02.118722Z","iopub.execute_input":"2025-02-16T05:32:02.119021Z","iopub.status.idle":"2025-02-16T05:32:02.131793Z","shell.execute_reply.started":"2025-02-16T05:32:02.118999Z","shell.execute_reply":"2025-02-16T05:32:02.131109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adan Optimizer","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Adan(Optimizer):\n    \"\"\"\n    Implements a pytorch variant of Adan\n    Adan was proposed in\n    Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.\n    https://arxiv.org/abs/2208.06677\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float, flot], optional): coefficients used for computing \n            running averages of gradient and its norm. (default: (0.98, 0.92, 0.99))\n        eps (float, optional): term added to the denominator to improve \n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): decoupled weight decay (L2 penalty) (default: 0)\n        max_grad_norm (float, optional): value used to clip \n            global grad norm (default: 0.0 no clip)\n        no_prox (bool): how to perform the decoupled weight decay (default: False)\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.98, 0.92, 0.99), eps=1e-8,\n                 weight_decay=0.2, max_grad_norm=0.0, no_prox=False):\n        if not 0.0 <= max_grad_norm:\n            raise ValueError(\"Invalid Max grad norm: {}\".format(max_grad_norm))\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= betas[2] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 2: {}\".format(betas[2]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm, no_prox=no_prox)\n        super(Adan, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(Adan, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('no_prox', False)\n\n    @torch.no_grad()\n    def restart_opt(self):\n        for group in self.param_groups:\n            group['step'] = 0\n            for p in group['params']:\n                if p.requires_grad:\n                    state = self.state[p]\n                    # State initialization\n\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    # Exponential moving average of gradient difference\n                    state['exp_avg_diff'] = torch.zeros_like(p)\n\n    @torch.no_grad()\n    def step(self):\n        \"\"\"\n            Performs a single optimization step.\n        \"\"\"\n        if self.defaults['max_grad_norm'] > 0:\n            device = self.param_groups[0]['params'][0].device\n            global_grad_norm = torch.zeros(1, device=device)\n\n            max_grad_norm = torch.tensor(self.defaults['max_grad_norm'], device=device)\n            for group in self.param_groups:\n\n                for p in group['params']:\n                    if p.grad is not None:\n                        grad = p.grad\n                        global_grad_norm.add_(grad.pow(2).sum())\n\n            global_grad_norm = torch.sqrt(global_grad_norm)\n\n            clip_global_grad_norm = torch.clamp(max_grad_norm / (global_grad_norm + group['eps']), max=1.0)\n        else:\n            clip_global_grad_norm = 1.0\n\n        for group in self.param_groups:\n            beta1, beta2, beta3 = group['betas']\n            # assume same step across group now to simplify things\n            # per parameter step can be easily support by making it tensor, or pass list into kernel\n            if 'step' in group:\n                group['step'] += 1\n            else:\n                group['step'] = 1\n\n            bias_correction1 = 1.0 - beta1 ** group['step']\n\n            bias_correction2 = 1.0 - beta2 ** group['step']\n\n            bias_correction3 = 1.0 - beta3 ** group['step']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state['exp_avg'] = torch.zeros_like(p)\n                    state['exp_avg_sq'] = torch.zeros_like(p)\n                    state['exp_avg_diff'] = torch.zeros_like(p)\n\n                grad = p.grad.mul_(clip_global_grad_norm)\n                if 'pre_grad' not in state or group['step'] == 1:\n                    state['pre_grad'] = grad\n\n                copy_grad = grad.clone()\n\n                exp_avg, exp_avg_sq, exp_avg_diff = state['exp_avg'], state['exp_avg_sq'], state['exp_avg_diff']\n                diff = grad - state['pre_grad']\n\n                update = grad + beta2 * diff\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)  # m_t\n                exp_avg_diff.mul_(beta2).add_(diff, alpha=1 - beta2)  # diff_t\n                exp_avg_sq.mul_(beta3).addcmul_(update, update, value=1 - beta3)  # n_t\n\n                denom = ((exp_avg_sq).sqrt() / math.sqrt(bias_correction3)).add_(group['eps'])\n                update = ((exp_avg / bias_correction1 + beta2 * exp_avg_diff / bias_correction2)).div_(denom)\n\n                if group['no_prox']:\n                    p.data.mul_(1 - group['lr'] * group['weight_decay'])\n                    p.add_(update, alpha=-group['lr'])\n                else:\n                    p.add_(update, alpha=-group['lr'])\n                    p.data.div_(1 + group['lr'] * group['weight_decay'])\n\n                state['pre_grad'] = copy_grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:32:02.132597Z","iopub.execute_input":"2025-02-16T05:32:02.132883Z","iopub.status.idle":"2025-02-16T05:32:02.149295Z","shell.execute_reply.started":"2025-02-16T05:32:02.132856Z","shell.execute_reply":"2025-02-16T05:32:02.148501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    scaler = torch.amp.GradScaler('cuda', enabled=CFG.apex)\n    losses = AverageMeter()\n    start = end = time.time()\n    global_step = 0\n    for step, batch in enumerate(train_loader):\n        precipitation = batch['precipitation'].to(device)\n        labels = batch['label'].to(device)\n        batch_size = labels.size(0)\n        with torch.amp.autocast('cuda', enabled=CFG.apex):\n            \n            pred_precipitation = model(precipitation)\n            \n            #loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n            loss = criterion(pred_precipitation.squeeze(1), labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        scaler.scale(loss).backward()\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            global_step += 1\n            if CFG.batch_scheduler:\n                scheduler.step()\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.8f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] loss\": losses.val,\n                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n    return losses.avg\n\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    targets = []\n    start = end = time.time()\n    for step, batch in enumerate(valid_loader):\n        precipitation = batch['precipitation'].to(device)\n        labels = batch['label'].to(device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            \n            pred_precipitation = model(precipitation)\n            \n            #loss = criterion(F.log_softmax(y_preds, dim=1), labels)\n            loss = criterion(pred_precipitation.squeeze(1), labels)\n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        losses.update(loss.item(), batch_size)\n        preds.append(pred_precipitation.sigmoid().flatten().detach().cpu().numpy())\n        targets.append(labels.flatten().detach().cpu().numpy())\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    targets = np.concatenate(targets)\n    return losses.avg, predictions, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:32:02.150107Z","iopub.execute_input":"2025-02-16T05:32:02.150364Z","iopub.status.idle":"2025-02-16T05:32:02.166104Z","shell.execute_reply.started":"2025-02-16T05:32:02.150335Z","shell.execute_reply":"2025-02-16T05:32:02.165507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    train_folds = folds[(folds['fold'] != fold)].reset_index(drop=True)\n    \n    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n    \n    train_dataset = CustomDataset(train_folds, augment=False, mode='train')\n    valid_dataset = CustomDataset(valid_folds, augment=False, mode=\"train\")\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n\n    model = WAVELT().to(device)\n    model.to(device)\n    if CFG.t4_gpu:\n        model = nn.DataParallel(model)\n    \n    def build_optimizer(cfg, optimizer_parameters, device):\n        lr = cfg.lr\n        # lr = default_configs[\"lr\"]\n        if cfg.optimizer == \"SAM\":\n            base_optimizer = torch.optim.SGD  # define an optimizer for the \"sharpness-aware\" update\n            optimizer_model = SAM(model.parameters(), base_optimizer, lr=lr, momentum=0.9, weight_decay=cfg.weight_decay, adaptive=True)\n        elif cfg.optimizer == \"Ranger21\":\n            optimizer_model = Ranger21(model.parameters(), lr=lr, weight_decay=cfg.weight_decay, \n            num_epochs=cfg.epochs, num_batches_per_epoch=len(train_loader))\n        elif cfg.optimizer == \"SGD\":\n            optimizer_model = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=cfg.weight_decay, momentum=0.9)\n        elif cfg.optimizer == \"Adam\":\n            optimizer_model = Adam(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n        elif cfg.optimizer == \"AdamW\":\n            optimizer_model = AdamW(model.parameters(), lr=lr, weight_decay=CFG.weight_decay)\n        elif cfg.optimizer == \"Lion\":\n            optimizer_model = Lion(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n        elif cfg.optimizer == \"Adan\":\n            optimizer_model = Adan(optimizer_parameters, lr=lr)\n    \n        return optimizer_model\n\n    def get_optimizer_params(model, lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        \n        optimizer_parameters = [\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': lr, 'weight_decay': 0.0}\n        ]\n    \n        return optimizer_parameters\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                CFG.lr,\n                                                weight_decay=CFG.weight_decay)\n    \n    optimizer = build_optimizer(CFG, optimizer_parameters, device)\n    \n    # ====================================================\n    # scheduler\n    # ====================================================\n    # ====================================================\n\n    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, **CFG.reduce_params)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, **CFG.cosanneal_params)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, **CFG.cosanneal_res_params)\n        elif CFG.scheduler=='OneCycleLR':\n            steps_per_epoch=len(train_loader),\n            scheduler = OneCycleLR(optimizer=optimizer, epochs=CFG.epochs, anneal_strategy=\"cos\", pct_start=0.05, steps_per_epoch=len(train_loader),\n        max_lr=CFG.lr, final_div_factor=100)\n        return scheduler\n    \n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss()\n\n    \n    best_score = np.inf\n\n    for epoch in range(CFG.epochs):\n\n        start_time = time.time()\n\n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, predictions, targets = valid_fn(valid_loader, model, criterion, device)\n        score = get_score(predictions, targets)\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  logloss: {score}  time: {elapsed:.0f}s')\n        if CFG.wandb:\n            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n                       f\"[fold{fold}] score\": score})\n        \n        if best_score > score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best valid logloss: {score:.7f} Model')\n            # CPMP: save the original model. It is stored as the module attribute of the DP model.\n            \n            state_dict = model.module.state_dict() if CFG.t4_gpu else model.state_dict()\n            torch.save({'model': state_dict,\n                            'predictions': predictions},\n                             OUTPUT_DIR+f\"{CFG.model_name}_fold{fold}_best_version{VERSION}.pth\")\n                \n    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model_name}_fold{fold}_best_version{VERSION}.pth\", \n                             map_location=torch.device('cpu'))['predictions']\n    \n    valid_folds[\"wavelt_oof_preds\"] = predictions\n    torch.cuda.empty_cache()\n    for i in range(100):\n        _ = gc.collect()\n    \n    return valid_folds, best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:32:02.16681Z","iopub.execute_input":"2025-02-16T05:32:02.167093Z","iopub.status.idle":"2025-02-16T05:32:02.185271Z","shell.execute_reply.started":"2025-02-16T05:32:02.167064Z","shell.execute_reply":"2025-02-16T05:32:02.184506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    \n    if CFG.train:\n        if CFG.wandb:\n            run = wandb.init(project='Inundata competition', \n                         name=CFG.model_name + f'version{VERSION}',\n                         config=class2dict(CFG),\n                         group=CFG.model_name,\n                         job_type=\"train\",\n                         )\n        \n        oof_df = pd.DataFrame()\n        scores = []\n        for fold in range(CFG.n_fold):\n            if fold in CFG.trn_fold:\n                _oof_df, score = train_loop(data, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                scores.append(score)\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                LOGGER.info(f'Score with best logloss weights: {score}')\n        oof_df = oof_df.reset_index(drop=True)\n        LOGGER.info(f\"========== CV ==========\")\n        LOGGER.info(f'Score with best logloss weights: {np.mean(scores)}')\n        oof_df.to_csv(OUTPUT_DIR+f'{CFG.model_name}_oof_df_version{VERSION}.csv', index=False)\n        \n    if CFG.wandb:\n        wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:32:02.185961Z","iopub.execute_input":"2025-02-16T05:32:02.186251Z","iopub.status.idle":"2025-02-16T05:34:56.532694Z","shell.execute_reply.started":"2025-02-16T05:32:02.186226Z","shell.execute_reply":"2025-02-16T05:34:56.53194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Normalizing the Predictions Based on the Flood Probability","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\n\nprint(f\"logloss before normalizing: {log_loss(oof_df['label'], oof_df['wavelt_oof_preds'])}\")\n\nlocations_to_normalize = oof_df[oof_df['flood_probability'] >= 0.5]['location_id'].unique()\noof_df['oof_sum_prob'] = oof_df.groupby('location_id')['wavelt_oof_preds'].transform('sum')\n\n# Avoid division by zero\nepsilon = 1e-8\noof_df['oof_wavelt_norm'] = oof_df['wavelt_oof_preds']  # Copy original values\n\noof_df.loc[oof_df['location_id'].isin(locations_to_normalize), 'oof_wavelt_norm'] = (\n    oof_df.loc[oof_df['location_id'].isin(locations_to_normalize), 'wavelt_oof_preds'] /\n    (oof_df.loc[oof_df['location_id'].isin(locations_to_normalize), 'oof_sum_prob'] + epsilon)\n)\n\nprint(f\"logloss after normalizing: {log_loss(oof_df['label'], oof_df['wavelt_oof_preds'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:35:36.55849Z","iopub.execute_input":"2025-02-16T05:35:36.558794Z","iopub.status.idle":"2025-02-16T05:35:36.607704Z","shell.execute_reply.started":"2025-02-16T05:35:36.558769Z","shell.execute_reply":"2025-02-16T05:35:36.606947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test_dataset = CustomDataset(data_test, augment=False, mode=\"test\")\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, drop_last=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:35:40.935461Z","iopub.execute_input":"2025-02-16T05:35:40.935767Z","iopub.status.idle":"2025-02-16T05:35:40.953392Z","shell.execute_reply.started":"2025-02-16T05:35:40.935743Z","shell.execute_reply":"2025-02-16T05:35:40.952498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(test_loader, model, device):\n    model.eval()\n    preds = []\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    for step, batch in tk0:\n        precipitation = batch['precipitation'].to(device)\n        batch_size = precipitation.size(0)\n        with torch.no_grad():\n            pred_precipitations = model(precipitation)\n        \n        preds.append(pred_precipitations.sigmoid().flatten().detach().cpu().numpy())\n\n    predictions = np.concatenate(preds)\n    \n    return predictions ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:35:53.580681Z","iopub.execute_input":"2025-02-16T05:35:53.581009Z","iopub.status.idle":"2025-02-16T05:35:53.586221Z","shell.execute_reply.started":"2025-02-16T05:35:53.58098Z","shell.execute_reply":"2025-02-16T05:35:53.585271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\nfor fold in CFG.trn_fold:\n    model = WAVELT().to(device)\n    state = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_version{VERSION}.pth', map_location=device)['model']\n    model.load_state_dict(state)\n    if CFG.t4_gpu:\n        model = nn.DataParallel(model)\n\n    prediction = inference(test_loader, model, device)\n    predictions.append(prediction)\npredictions = np.mean(predictions, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:35:56.897546Z","iopub.execute_input":"2025-02-16T05:35:56.897916Z","iopub.status.idle":"2025-02-16T05:36:44.31797Z","shell.execute_reply.started":"2025-02-16T05:35:56.897881Z","shell.execute_reply":"2025-02-16T05:36:44.317277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission = pd.read_csv(BASE_PATH + '/SampleSubmission.csv')\nsample_submission['wavelt_preds'] = predictions\nsample_submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:37:00.484499Z","iopub.execute_input":"2025-02-16T05:37:00.484807Z","iopub.status.idle":"2025-02-16T05:37:00.631799Z","shell.execute_reply.started":"2025-02-16T05:37:00.484782Z","shell.execute_reply":"2025-02-16T05:37:00.63098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.to_csv(f'{CFG.model_name}_submission_version{VERSION}.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:37:02.932545Z","iopub.execute_input":"2025-02-16T05:37:02.932858Z","iopub.status.idle":"2025-02-16T05:37:03.251294Z","shell.execute_reply.started":"2025-02-16T05:37:02.932833Z","shell.execute_reply":"2025-02-16T05:37:03.25038Z"}},"outputs":[],"execution_count":null}]}